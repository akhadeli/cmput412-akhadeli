1:"$Sreact.fragment"
2:I[2034,["173","static/chunks/173-7be02dd55462ff0c.js","678","static/chunks/678-9555f6ed956a8d40.js","177","static/chunks/app/layout-ebf74d6892f2c1b6.js"],"SidebarProvider"]
3:I[1543,["173","static/chunks/173-7be02dd55462ff0c.js","678","static/chunks/678-9555f6ed956a8d40.js","177","static/chunks/app/layout-ebf74d6892f2c1b6.js"],"AppSidebar"]
4:I[2034,["173","static/chunks/173-7be02dd55462ff0c.js","678","static/chunks/678-9555f6ed956a8d40.js","177","static/chunks/app/layout-ebf74d6892f2c1b6.js"],"SidebarTrigger"]
5:I[5244,[],""]
6:I[3866,[],""]
8:I[6213,[],"OutletBoundary"]
a:I[6213,[],"MetadataBoundary"]
c:I[6213,[],"ViewportBoundary"]
e:I[4835,[],""]
:HL["/cmput412-akhadeli/_next/static/media/a34f9d1faa5f3315-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/cmput412-akhadeli/_next/static/css/f8c155f11af27ff1.css","style"]
:HL["/cmput412-akhadeli/_next/static/css/e23141aac3fba32d.css","style"]
7:T456,prose prose-headings:mt-8 prose-headings:font-semibold prose-headings:text-black prose-h1:text-5xl prose-h2:text-4xl prose-h3:text-3xl prose-h4:text-2xl prose-h5:text-xl prose-h6:text-lg dark:prose-headings:text-white prose-md prose-p:text-lg prose-p:leading-relaxed prose-p:text-gray-800 dark:prose-p:text-gray-100 prose-table:w-full prose-table:border-collapse prose-table:border-gray-300 prose-table:dark:border-gray-600 prose-table:text-left prose-table:text-sm prose-table:text-gray-800 dark:prose-table:text-gray-100 prose-ul:text-gray-800 dark:prose-ul:text-gray-100 prose-li:text-gray-800 dark:prose-li:text-gray-100 prose-strong:text-gray-800 prose-strong:font-bold dark:prose-strong:text-gray-100 prose-img:w-full prose-img:rounded-lg prose-img:border prose-img:border-border prose-img:dark:border-border prose-video:w-full prose-video:rounded-lg prose-video:border prose-video:border-border prose-video:dark:border-border prose-iframe:w-full prose-iframe:rounded-lg prose-iframe:border prose-iframe:border-border prose-iframe:dark:border-border prose-span:text-gray-800 prose-span:dark:text-gray-1000:{"P":null,"b":"IyTIDrFCTj8nxwzP0krqc","p":"/cmput412-akhadeli","c":["","exercises","ex4"],"i":false,"f":[[["",{"children":["exercises",{"children":["ex4",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/cmput412-akhadeli/_next/static/css/f8c155f11af27ff1.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":["$","body",null,{"className":"__className_23f766 antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{}],["$","$L4",null,{"className":"ml-3 mt-3"}],["$","main",null,{"className":"flex-1 overflow-auto p-8 pt-16","children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[],[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}]}]}]]}],{"children":["exercises",["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","exercises","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["ex4",["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","exercises","children","ex4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","main",null,{"className":"mx-auto max-w-3xl","children":["$","div",null,{"className":"$7","children":[["$","h1",null,{"children":"CMPUT 412/503 - Exercise 4 Report"}],"\n",["$","h2",null,{"children":"Abdullah Khadeli and Ryan Rom"}],"\n",["$","h3",null,{"children":"Part I - AprilTag Detection"}],"\n",["$","h4",null,{"children":"Camera Calibration and Image Processing"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Camera Undistortion"}],": We used our camera calibration file to undistort the images captured by the Duckiebot's camera, using the same approach as in Exercise 3:"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"# Subscribing to camera topic\nself.sub = rospy.Subscriber(self._camera_topic, CompressedImage, self.callback)\n\n# In the callback function\nimage = self._bridge.compressed_imgmsg_to_cv2(msg)\nundistorted_image = cv2.undistort(image, camera_matrix, dist_coeffs)\n"}]}],"\n",["$","p",null,{"children":"We used the following camera intrinsics for undistortion:"}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"# Camera intrinsics\nK = np.array([[310.0149584021843, 0.0, 307.7177249518777],\n            [0.0, 309.29643750324607, 229.191787718834],\n            [0.0, 0.0, 1.0]], dtype=np.float32)\n\nD = np.array([-0.2683225140828933, 0.049595473114203516,\n            0.0003617920649662741, 0.006030049583437601, 0.0], dtype=np.float32)\n\n# Compute optimal camera matrix to minimize black areas after undistortion\nnew_K, roi = cv2.getOptimalNewCameraMatrix(K, D, (img_width, img_height), 1, (img_width, img_height))\n\n# Undistort image\nundistorted = cv2.undistort(image, K, D, None, new_K)\n"}]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Image Preprocessing"}],":"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"We used the optimal camera matrix to minimize black areas after undistortion"}],"\n",["$","li",null,{"children":"Cropped the image based on ROI (Region of Interest) after undistortion"}],"\n",["$","li",null,{"children":"Converted the image to grayscale for processing with the AprilTag detector"}],"\n",["$","li",null,{"children":"For better detection, we focused on the right half of the undistorted image:"}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"def publish_undistort_grayscale(self, undistort):\n    image = undistort\n    # Convert to grayscale and store it in grayscale_image\n    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Get image dimensions\n    height, width = grayscale_image.shape\n\n    # Crop the right half of grayscale_image\n    grayscale_image = grayscale_image[:, width // 2:]\n"}]}],"\n",["$","p",null,{"children":"Our preprocessing improved detection by:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Cropping the right half"}],": Most tags were on the right side of the road, reducing computational load and false positives"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Grayscale conversion"}],": Reduced data dimensionality while preserving tag contrast, improving speed by ~30%"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Optimal camera matrix"}],": Preserved field of view while correcting lens distortions"]}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","h4",null,{"children":"AprilTag Detection Implementation"}],"\n",["$","p",null,{"children":["We implemented AprilTag detection using the ",["$","code",null,{"children":"dt_apriltags"}]," library, which allows us to detect the tag36h11 family of AprilTags:"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"from dt_apriltags import Detector\n\n# Initialize AprilTag detector\ndetector = dt_apriltags.Detector(families=\"tag36h11\")\n\n# Detect AprilTags in the image\nresults = detector.detect(gray)\n"}]}],"\n",["$","p",null,{"children":["Our implementation identifies three specific AprilTag IDs, defined in our ",["$","code",null,{"children":"apriltags.py"}]," file:"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class AprilTag(Enum):\n    UALBERTA = 201\n    INTERSECTIONT = 133\n    STOP = 21\n"}]}],"\n",["$","p",null,{"children":"We implemented our detection with a rate of 10Hz, which provided a good balance between:"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":"Avoid excessive overhead and overloading the Duckiebot's processor."}],"\n",["$","li",null,{"children":"Sufficient frequency to ensure consistent detection of tags even at moderate speeds."}],"\n"]}],"\n",["$","p",null,{"children":"For each detected tag, we:"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":"Identified the specific tag type based on its ID."}],"\n",["$","li",null,{"children":"Published the state to a ROS topic for other nodes to react to."}],"\n"]}],"\n",["$","p",null,{"children":"Our 10Hz detection rate was chosen based on:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Robot speed"}],": At about 0.3 m/s, this checks for tags every ~3cm, ensuring reliable detection"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Experimental testing"}],": 5Hz missed tags at full speed, >20Hz caused lag in other functions"]}],"\n"]}],"\n",["$","h4",null,{"children":"LED Control Based on AprilTag Detection"}],"\n",["$","p",null,{"children":["We implemented LED color changes based on the detected AprilTag using a dedicated ",["$","code",null,{"children":"LEDControl"}]," node that subscribes to the robot's state:"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class LEDControl(DTROS):\n    def __init__(self, node_name):\n        # ...\n        self.states = [\n            State(message_name=\"No tag detected\", colorPattern=ColorPattern(frontLeft=Colors.White, frontRight=Colors.White, backLeft=Colors.White, backRight=Colors.White)),\n            State(message_name=\"INTERSECTIONT tag detected\", colorPattern=ColorPattern(frontLeft=Colors.Blue, frontRight=Colors.Blue, backLeft=Colors.Blue, backRight=Colors.Blue)),\n            State(message_name=\"STOP tag detected\", colorPattern=ColorPattern(frontLeft=Colors.Red, frontRight=Colors.Red, backLeft=Colors.Red, backRight=Colors.Red)),\n            State(message_name=\"UALBERTA tag detected\", colorPattern=ColorPattern(frontLeft=Colors.Green, frontRight=Colors.Green, backLeft=Colors.Green, backRight=Colors.Green)),\n            # Additional states for other behaviors\n        ]\n"}]}],"\n",["$","p",null,{"children":"This implementation sets:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Red"}],": When a Stop Sign tag (ID: 21) is detected"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Blue"}],": When a T-Intersection tag (ID: 133) is detected"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Green"}],": When a UofA tag (ID: 201) is detected"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"White"}],": Default state (no detection)"]}],"\n"]}],"\n",["$","h4",null,{"children":"Intersection Behavior"}],"\n",["$","p",null,{"children":"When approaching an intersection, our Duckiebot:"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":"Detects red intersection lines using a color detection approach similar to Exercise 3"}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":["Stops for a duration based on the last seen AprilTag, implemented in the ",["$","code",null,{"children":"StopBehavior"}]," class:"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class StopBehavior():\n    def __init__(self, red_detection_magnitude, stop_time=3):\n        self.detection_magnitude=red_detection_magnitude\n        self.stop_time = stop_time\n\n    def execute(self, dtros):\n        # Find red line to stop\n        # ...\n\n        # Stop for the appropriate time based on tag\n        message = WheelsCmdStamped(vel_left=0, vel_right=0)\n        dtros._publisher.publish(message)\n        rospy.sleep(self.stop_time)\n"}]}],"\n",["$","p",null,{"children":"With stop times of:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Stop Sign (ID: 21): 3 seconds"}],"\n",["$","li",null,{"children":"T-Intersection (ID: 133): 2 seconds"}],"\n",["$","li",null,{"children":"UofA Tag (ID: 201): 1 second"}],"\n",["$","li",null,{"children":"No detection (default): 0.5 seconds"}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","p",null,{"children":"We integrated AprilTag detection with intersection behavior through:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Parallel processing with separate nodes for detection and intersection monitoring"}],"\n",["$","li",null,{"children":["A \"tag memory\" system that remembers the last detected tag:","\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"def tag_detection_callback(self, msg):\n    self.last_detected_tag = msg.data\n    self.last_detection_time = rospy.get_time()\n"}]}],"\n"]}],"\n",["$","li",null,{"children":"State-based decision making to determine stop time at intersections"}],"\n"]}],"\n",["$","video",null,{"src":"/cmput412-akhadeli/images/ex4/apriltag_screencast.webm","controls":true}],"\n",["$","p",null,{"children":"This screencast demonstrates our AprilTag detection system in action. In the rqt_image_view, we see successful identification of all three types of tags: Stop Sign (ID: 21), T-Intersection (ID: 133), and UofA (ID: 201). Each tag is highlighted with a colored bounding box and labeled with its ID number. We also observe that detections can be made at different angles, and multiple tags can be detected at the same time."}],"\n",["$","hr",null,{}],"\n",["$","video",null,{"src":"/cmput412-akhadeli/images/ex4/apriltag_led.mp4","controls":true}],"\n",["$","p",null,{"children":"This video showcases our Duckiebot responding to different AprilTags by changing its LED lights accordingly. When it detects a Stop Sign tag (ID: 21), the LEDs turn red. For T-Intersection tags (ID: 133), they turn blue. When seeing a UofA tag (ID: 201), they turn green. And when no tag is in view, the LEDs remain white. We can also see that the robot stops at the intersection for the respective amount of time."}],"\n",["$","hr",null,{}],"\n",["$","h3",null,{"children":"Part II - PeDuckstrian Crosswalks"}],"\n",["$","h4",null,{"children":"Crosswalk Detection"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Detecting Blue Crosswalk Lines"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"# Blue tape detection for crosswalks\nlower_blue = np.array([100, 150, 0], dtype=np.uint8)\nupper_blue = np.array([140, 255, 255], dtype=np.uint8)\nblue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n\n# Apply dilation to improve detection\nkernel = np.ones((5, 5), \"uint8\")\nblue_mask = cv2.dilate(blue_mask, kernel)\n\n# Find contours for blue lines\ncontours, _ = cv2.findContours(blue_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n"}]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Handling Double Blue Lines"}],":"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["We implemented a task-based approach to handle crosswalks with the ",["$","code",null,{"children":"PDCrossWalkMain"}]," class"]}],"\n",["$","li",null,{"children":["The robot stops at the first blue line using our ",["$","code",null,{"children":"FindCrossWalk"}]," task, which detects blue lines with a magnitude threshold of 3000 pixels"]}],"\n",["$","li",null,{"children":"We determine when to stop by calculating the distance from the bottom of the frame to the closest blue line pixels:"}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"def getCenterDistanceFromBottom(self, mask):\n    y_coords, x_coords = np.where(mask > 0)\n    active_pixel_count = len(x_coords)\n\n    if active_pixel_count < self.detection_magnitude:\n        return math.inf  # Not enough active pixels\n\n    # Compute distance from the bottom of the image\n    img_height = mask.shape[0]\n    distance_from_bottom = img_height - max(y_coords)\n\n    return distance_from_bottom\n"}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["After stopping and checking for peDuckstrians, we use the ",["$","code",null,{"children":"DrivePastCrossWalk"}]," task to proceed through the crosswalk, defining \"past the crosswalk\" as when the blue detection magnitude falls below our threshold:"]}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"def isDetectingCrossWalk(self, homography_mask):\n    active_pixel_count = np.count_nonzero(homography_mask > 0)\n\n    if active_pixel_count > self.detection_magnitude:\n        return True  # Crosswalk detected\n    return False  # No crosswalk detected\n"}]}],"\n"]}],"\n"]}],"\n",["$","p",null,{"children":"We handled double blue lines through a state-based approach:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Used a state machine to track progress (looking for first line → waiting/processing → driving past both lines)"}],"\n",["$","li",null,{"children":["Implemented a \"blue line memory\" flag after stopping at the first line:","\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"if self.has_stopped_at_crosswalk:\n    drive_forward()  # Already stopped, continue through second line\nelse:\n    stop()\n    self.has_stopped_at_crosswalk = True  # First line encountered\n"}]}],"\n"]}],"\n",["$","li",null,{"children":"Used distance-based identification to determine the first line"}],"\n",["$","li",null,{"children":"Reset the state after passing both lines, preparing for the next crosswalk"}],"\n"]}],"\n",["$","h4",null,{"children":"PeDuckstrian Detection"}],"\n",["$","p",null,{"children":"To detect peDuckstrians crossing at crosswalks:"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":"Our implementation looks for yellow objects in the homography-transformed image:"}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"def isPeduckstriansVisible(self, mask):\n    _, x_coords = np.where(mask > 0)\n\n    if len(x_coords) == 0:\n        return False\n\n    x_variance = np.var(x_coords)\n    magnitude = len(x_coords)\n\n    if (magnitude >= self.detection_magnitude):\n        if(x_variance >= self.detection_variance):\n            print(\"Peduckstrians detected\")\n            return True\n    return False\n"}]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Crosswalk Behavior"}],":"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"We use a sequence of tasks to handle the crosswalk behavior:"}],"\n"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"tasks = [\n    FindCrossWalk(detection_magnitude=3000, stopping_distance_in_pixels=5),\n    WaitForPeduckstrians(detection_magnitude=2000, detection_variance=5000),\n    Stop(stop_time=1),\n    DrivePastCrossWalk(detection_magnitude=3000),\n    # ... repeated for multiple crosswalks\n]\n"}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"By sequencing the tasks, we exit the find crosswalk task to then detect and wait for peDuckstrians to clear the crosswalk"}],"\n",["$","li",null,{"children":["The ",["$","code",null,{"children":"WaitForPeduckstrians"}]," task checks if peDuckstrians are present by analyzing both the magnitude and variance in the yellow mask"]}],"\n",["$","li",null,{"children":"If the magnitude and variance thresholds are met, the robot waits until peDuckstrians have cleared"}],"\n",["$","li",null,{"children":"After peDuckstrians have cleared or if none are detected, the robot stops for 1 second, then proceeds through the crosswalk"}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","p",null,{"children":"Detection method:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Color filtering"}],": Used HSV thresholds to isolate yellow objects"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Magnitude threshold"}],": Required at least 2000 yellow pixels to indicate a significant object"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Variance analysis"}],": Measured the spread of yellow pixels (variance > 5000) to distinguish between:","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"PeDuckstrian strips (high variance, spread across road)"}],"\n",["$","li",null,{"children":"Other yellow objects (typically lower variance)"}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","p",null,{"children":"Potential failure scenarios include:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Extreme lighting affecting color detection"}],"\n",["$","li",null,{"children":"Partial visibility of strips at frame edges"}],"\n",["$","li",null,{"children":"Other yellow objects causing false positives"}],"\n",["$","li",null,{"children":"Occlusion by other objects"}],"\n"]}],"\n",["$","p",null,{"children":"Experimental testing of magnitude and variance thresholds helped us select values which mitigated these failure scenarios."}],"\n",["$","video",null,{"src":"/cmput412-akhadeli/images/ex4/crosswalk_video.mp4","controls":true}],"\n",["$","p",null,{"children":"This video demonstrates our Duckiebot's behavior at crosswalks. Initially, the Duckiebot approaches an empty crosswalk, stops for 1 second at the first blue line, and then proceeds. Next, when it encounters a crosswalk with peDuckstrians, it waits until they've cleared before continuing. The Duckiebot successfully distinguishes between the two cases, stopping only at the first blue line and waiting appropriately when peDuckstrians are present."}],"\n",["$","hr",null,{}],"\n",["$","h3",null,{"children":"Part III - Safe Navigation"}],"\n",["$","h4",null,{"children":"Broken-Down Duckiebot Detection"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Detection Approach"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"def blue_detection_callback(self, msg):\n    # msg is already a mask\n    mask_blue = self._bridge.compressed_imgmsg_to_cv2(msg)\n\n    # Find the coordinates of active (non-zero) pixels\n    y_coords, x_coords = np.where(mask_blue > 0)  # y, x positions of active pixels\n\n    if len(x_coords) == 0:\n        return\n\n    # Calculate the variance of the x coordinates\n    x_variance = np.var(x_coords)\n\n    # Calculate the magnitude (number of active blue pixels)\n    magnitude = len(x_coords)\n\n    if (magnitude >= self.duckie_detection_sensitivity):\n        if(x_variance <= self.duckie_detection_distance):\n            self._duckie_detected = True\n            self._duckie_detected_time_stamp = time.time()\n            return\n"}]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Safe Distance Stopping"}],":"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"We detect the broken-down Duckiebot using blue color detection."}],"\n",["$","li",null,{"children":["Our approach uses two key parameters:","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","code",null,{"children":"duckie_detection_sensitivity"}],": Threshold for the number of blue pixels (set to 2000)"]}],"\n",["$","li",null,{"children":[["$","code",null,{"children":"duckie_detection_distance"}],": Maximum variance of blue pixels (set to 30000)"]}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":"When blue pixels with high magnitude and low variance are detected, we consider it to be a broken-down Duckiebot"}],"\n",["$","li",null,{"children":"The robot automatically stops when it detects the broken-down Duckiebot and sets a timestamp for lane change planning."}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","p",null,{"children":"Our method for detecting broken-down Duckiebots used a color-based approach with geometric constraints:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Targeted the blue shell as the most distinctive feature"}],"\n",["$","li",null,{"children":["Used dual parameters for reliable detection:","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Magnitude threshold (2000 pixels) to ensure sufficient size"}],"\n",["$","li",null,{"children":"Variance constraint (below 30000) to verify compact clustering"}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":"Required persistent detection across multiple frames to reduce false positives"}],"\n"]}],"\n",["$","p",null,{"children":"We tried several other approaches before settling on this method:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["Tracking based detection using Lucas-Kanade optical flow","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"We found it hard to implement and time consuming to get working"}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","p",null,{"children":"Our final approach provided the best balance of reliability and computational efficiency."}],"\n",["$","h4",null,{"children":"Navigation Around Obstacles"}],"\n",["$","p",null,{"children":"We implemented a task-based approach for safe navigation around broken-down Duckiebots with three main tasks:"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Vehicle Avoidance Detection"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class VehicleAvoidance(VehicleAvoidanceTask):\n    def runTask(self, dtros):\n        print(\"VehicleAvoidance\")\n        rate = rospy.Rate(10)\n\n        while not rospy.is_shutdown():\n            correctionUpdate = dtros.getUpdate()\n\n            if(dtros._duckie_detected_time_stamp is not None and (time.time() - dtros._duckie_detected_time_stamp) < dtros.lane_correction_delay):\n                print(\"Duckie Detected\")\n                break\n\n            # Continue driving until detection\n            # ...\n"}]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Stop and Assess"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class Stop(VehicleAvoidanceTask):\n    def __init__(self, stop_time=3):\n        super().__init__()\n        self._stop_time = stop_time\n\n    def runTask(self, dtros):\n        print(\"Stopping\")\n        stop = WheelsCmdStamped(vel_left=0, vel_right=0)\n        dtros._publisher.publish(stop)\n        time.sleep(self._stop_time)\n"}]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Lane Change and Return"}],":"]}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"class SwitchToLeftLane(VehicleAvoidanceTask):\n    def runTask(self, dtros):\n        print(\"SwitchToLeftLane\")\n        rate = rospy.Rate(10)\n\n        while not rospy.is_shutdown():\n            correctionUpdate = dtros.getUpdate()\n\n            if dtros._duckie_detected_time_stamp is not None and (time.time() - dtros._duckie_detected_time_stamp) >= dtros.lane_correction_delay:\n                break\n\n            # Continue in left lane until timeout\n            # ...\n"}]}],"\n"]}],"\n"]}],"\n",["$","p",null,{"children":"After detecting the broken-down Duckiebot, the robot swaps its lane tracking targets:"}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"if self._duckie_detected and self._duckie_detected_time_stamp is not None and (time.time() - self._duckie_detected_time_stamp) < self.lane_correction_delay:\n    yellow_error = self.compute_error(mask=mask_yellow, target_x=489)\n    white_error = self.compute_error(mask=mask_white, target_x=100)\nelse:\n    yellow_error = self.compute_error(mask=mask_yellow, target_x=100)\n    white_error = self.compute_error(mask=mask_white, target_x=489)\n"}]}],"\n",["$","p",null,{"children":"This effectively inverts the lane-following algorithm, causing the robot to switch to the left lane while maintaining its ability to follow the lane markers."}],"\n",["$","p",null,{"children":"Our maneuvering approach leveraged our existing lane-following system with key additions:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Target position inversion"}],": Dynamically switched target positions for yellow and white lane markers","\n",["$","pre",null,{"children":["$","code",null,{"className":"language-python","children":"# Normal vs. overtaking lane targets\nyellow_error = self.compute_error(mask=mask_yellow, target_x=100)  # Normal: yellow on left\nyellow_error = self.compute_error(mask=mask_yellow, target_x=489)  # Overtaking: yellow on right\n"}]}],"\n"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Sequential execution"}],": Three-phase process (detection → assessment → avoidance)"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"PID controller reuse"}],": Same controller with different targets for smooth transitions"]}],"\n",["$","li",null,{"children":"This approach ensured stable lane changes while leveraging our well-tuned lane following system"}],"\n"]}],"\n",["$","video",null,{"src":"/cmput412-akhadeli/images/ex4/safe_navigation.mp4","controls":true}],"\n",["$","p",null,{"children":"This video showcases our Duckiebot's obstacle avoidance capabilities. It begins with the robot driving straight until it detects a broken-down Duckiebot in its path. Upon detection, our robot stops at a safe distance and pauses for 3 seconds. Then, it executes the lane change maneuver, navigating into the opposing lane to safely pass the obstacle. Once it has cleared the broken-down bot, our Duckiebot automatically returns to the original right lane and continues driving."}],"\n",["$","hr",null,{}],"\n",["$","h3",null,{"children":"Reflection"}],"\n",["$","p",null,{"children":"We found this exercise very insightful, as we got the opportunity to work with AprilTag detection and advanced navigation behaviors. The integration of multiple systems presented unique challenges that required careful coordination of perception and control components."}],"\n",["$","h3",null,{"children":"Code"}],"\n",["$","p",null,{"children":["Find the code using the link\n",["$","a",null,{"href":"https://github.com/akhadeli/cmput412-akhadeli-code/tree/main/exercise-4","children":"Code"}]]}],"\n",["$","h3",null,{"children":"References"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["$","a",null,{"href":"https://wiki.ros.org/ROS/Tutorials","children":"ROS Documentation"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://docs.duckietown.com/daffy/devmanual-software/beginner","children":"Duckiebot Documentation"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://medium.com/@nahmed3536/wheel-odometry-model-for-differential-drive-robotics-91b85a012299","children":"Odometry Medium Article"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://david010.medium.com/lane-tracking-via-computer-vision-2acb4c7c1c22","children":"Lane Tracking Article"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://docs.opencv.org/","children":"OpenCV Documentation"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/","children":"GeeksForGeeks Article"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://en.wikipedia.org/wiki/PID_controller","children":"PID Controller Wikipedia"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://www.youtube.com/watch?v=y3K6FUgrgXw","children":"PID Controllers in Unity"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://en.wikipedia.org/wiki/HSL_and_HSV","children":"HSV Color Space Wikipedia"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://github.com/duckietown/lib-dt-apriltags","children":"AprilTag Library Documentation"}]}],"\n",["$","li",null,{"children":["$","a",null,{"href":"https://april.eecs.umich.edu/software/apriltag","children":"April Robotics Laboratory"}]}],"\n"]}]]}]}],[["$","link","0",{"rel":"stylesheet","href":"/cmput412-akhadeli/_next/static/css/e23141aac3fba32d.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L8",null,{"children":"$L9"}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","8qbAYkTHnR8S_v1Yep-pl",{"children":[["$","$La",null,{"children":"$Lb"}],["$","$Lc",null,{"children":"$Ld"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]]}],false]],"m":"$undefined","G":["$e","$undefined"],"s":false,"S":true}
d:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"CMPUT 412/503 - Abdullah Khadeli"}],["$","meta","2",{"name":"description","content":"A documentation site for CMPUT 412/503 - Abdullah Khadeli"}],["$","link","3",{"rel":"icon","href":"/cmput412-akhadeli/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
9:null
