<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/cmput412-akhadeli/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/cmput412-akhadeli/_next/static/css/f8c155f11af27ff1.css" data-precedence="next"/><link rel="stylesheet" href="/cmput412-akhadeli/_next/static/css/e23141aac3fba32d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/cmput412-akhadeli/_next/static/chunks/webpack-9df4f922c7104209.js"/><script src="/cmput412-akhadeli/_next/static/chunks/4bd1b696-9924fae48e609361.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/517-88af3900d71f33ad.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/main-app-8535f9fd3beb3af1.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/173-7be02dd55462ff0c.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/678-9555f6ed956a8d40.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/app/layout-ebf74d6892f2c1b6.js" async=""></script><meta name="next-size-adjust" content=""/><title>CMPUT 412/503 - Abdullah Khadeli</title><meta name="description" content="A documentation site for CMPUT 412/503 - Abdullah Khadeli"/><link rel="icon" href="/cmput412-akhadeli/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/cmput412-akhadeli/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_23f766 antialiased"><div style="--sidebar-width:16rem;--sidebar-width-icon:3rem" class="group/sidebar-wrapper flex min-h-svh w-full has-[[data-variant=inset]]:bg-sidebar"><div class="group peer hidden text-sidebar-foreground md:block" data-state="expanded" data-collapsible="" data-variant="sidebar" data-side="left"><div class="relative h-svh w-[--sidebar-width] bg-transparent transition-[width] duration-200 ease-linear group-data-[collapsible=offcanvas]:w-0 group-data-[side=right]:rotate-180 group-data-[collapsible=icon]:w-[--sidebar-width-icon]"></div><div class="fixed inset-y-0 z-10 hidden h-svh w-[--sidebar-width] transition-[left,right,width] duration-200 ease-linear md:flex left-0 group-data-[collapsible=offcanvas]:left-[calc(var(--sidebar-width)*-1)] group-data-[collapsible=icon]:w-[--sidebar-width-icon] group-data-[side=left]:border-r group-data-[side=right]:border-l"><div data-sidebar="sidebar" class="flex h-full w-full flex-col bg-sidebar group-data-[variant=floating]:rounded-lg group-data-[variant=floating]:border group-data-[variant=floating]:border-sidebar-border group-data-[variant=floating]:shadow"><div data-sidebar="header" class="flex flex-col gap-2 p-2"><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="lg" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-12 text-sm group-data-[collapsible=icon]:!p-0" href="/cmput412-akhadeli"><div class="bg-primary text-primary-foreground flex aspect-square size-8 items-center justify-center rounded-lg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-gallery-vertical-end size-4"><path d="M7 2h10"></path><path d="M5 6h14"></path><rect width="18" height="12" x="3" y="10" rx="2"></rect></svg></div><div class="flex flex-col gap-0.5 leading-none"><span class="font-semibold">CMPUT 412 - akhadeli</span><span class="">v1.0.0</span></div></a></li></ul></div><div data-sidebar="content" class="flex min-h-0 flex-1 flex-col gap-2 overflow-auto group-data-[collapsible=icon]:overflow-hidden"><div data-sidebar="group" class="relative flex w-full min-w-0 flex-col p-2"><div data-sidebar="group-label" class="flex h-8 shrink-0 items-center rounded-md px-2 text-xs font-medium text-sidebar-foreground/70 outline-none ring-sidebar-ring transition-[margin,opa] duration-200 ease-linear focus-visible:ring-2 [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 group-data-[collapsible=icon]:-mt-8 group-data-[collapsible=icon]:opacity-0">Navigation</div><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli"><span>Home</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="https://github.com/akhadeli/cmput412-akhadeli-code"><span>Repository</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/contact"><span>Contact</span></a></li></ul></div><div data-sidebar="group" class="relative flex w-full min-w-0 flex-col p-2"><div data-sidebar="group-label" class="flex h-8 shrink-0 items-center rounded-md px-2 text-xs font-medium text-sidebar-foreground/70 outline-none ring-sidebar-ring transition-[margin,opa] duration-200 ease-linear focus-visible:ring-2 [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 group-data-[collapsible=icon]:-mt-8 group-data-[collapsible=icon]:opacity-0">Exercises</div><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/exercises/ex1"><span>Exercise 1</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/exercises/ex2"><span>Exercise 2</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/exercises/ex3"><span>Exercise 3</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/exercises/ex4"><span>Exercise 4</span></a></li></ul></div></div></div></div></div><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-7 w-7 ml-3 mt-3" data-sidebar="trigger"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-panel-left"><rect width="18" height="18" x="3" y="3" rx="2"></rect><path d="M9 3v18"></path></svg><span class="sr-only">Toggle Sidebar</span></button><main class="flex-1 overflow-auto p-8 pt-16"><main class="mx-auto max-w-3xl"><div class="prose prose-headings:mt-8 prose-headings:font-semibold prose-headings:text-black prose-h1:text-5xl prose-h2:text-4xl prose-h3:text-3xl prose-h4:text-2xl prose-h5:text-xl prose-h6:text-lg dark:prose-headings:text-white prose-md prose-p:text-lg prose-p:leading-relaxed prose-p:text-gray-800 dark:prose-p:text-gray-100 prose-table:w-full prose-table:border-collapse prose-table:border-gray-300 prose-table:dark:border-gray-600 prose-table:text-left prose-table:text-sm prose-table:text-gray-800 dark:prose-table:text-gray-100 prose-ul:text-gray-800 dark:prose-ul:text-gray-100 prose-li:text-gray-800 dark:prose-li:text-gray-100 prose-strong:text-gray-800 prose-strong:font-bold dark:prose-strong:text-gray-100 prose-img:w-full prose-img:rounded-lg prose-img:border prose-img:border-border prose-img:dark:border-border prose-video:w-full prose-video:rounded-lg prose-video:border prose-video:border-border prose-video:dark:border-border prose-iframe:w-full prose-iframe:rounded-lg prose-iframe:border prose-iframe:border-border prose-iframe:dark:border-border prose-span:text-gray-800 prose-span:dark:text-gray-100"><h1>CMPUT 412/503 - Exercise 4 Report</h1>
<h2>Abdullah Khadeli and Ryan Rom</h2>
<h3>Part I - AprilTag Detection</h3>
<h4>Camera Calibration and Image Processing</h4>
<ol>
<li>
<p><strong>Camera Undistortion</strong>: We used our camera calibration file to undistort the images captured by the Duckiebot&#x27;s camera, using the same approach as in Exercise 3:</p>
<pre><code class="language-python"># Subscribing to camera topic
self.sub = rospy.Subscriber(self._camera_topic, CompressedImage, self.callback)

# In the callback function
image = self._bridge.compressed_imgmsg_to_cv2(msg)
undistorted_image = cv2.undistort(image, camera_matrix, dist_coeffs)
</code></pre>
<p>We used the following camera intrinsics for undistortion:</p>
<pre><code class="language-python"># Camera intrinsics
K = np.array([[310.0149584021843, 0.0, 307.7177249518777],
            [0.0, 309.29643750324607, 229.191787718834],
            [0.0, 0.0, 1.0]], dtype=np.float32)

D = np.array([-0.2683225140828933, 0.049595473114203516,
            0.0003617920649662741, 0.006030049583437601, 0.0], dtype=np.float32)

# Compute optimal camera matrix to minimize black areas after undistortion
new_K, roi = cv2.getOptimalNewCameraMatrix(K, D, (img_width, img_height), 1, (img_width, img_height))

# Undistort image
undistorted = cv2.undistort(image, K, D, None, new_K)
</code></pre>
</li>
<li>
<p><strong>Image Preprocessing</strong>:</p>
<ul>
<li>We used the optimal camera matrix to minimize black areas after undistortion</li>
<li>Cropped the image based on ROI (Region of Interest) after undistortion</li>
<li>Converted the image to grayscale for processing with the AprilTag detector</li>
<li>For better detection, we focused on the right half of the undistorted image:</li>
</ul>
<pre><code class="language-python">def publish_undistort_grayscale(self, undistort):
    image = undistort
    # Convert to grayscale and store it in grayscale_image
    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Get image dimensions
    height, width = grayscale_image.shape

    # Crop the right half of grayscale_image
    grayscale_image = grayscale_image[:, width // 2:]
</code></pre>
<p>Our preprocessing improved detection by:</p>
<ul>
<li><strong>Cropping the right half</strong>: Most tags were on the right side of the road, reducing computational load and false positives</li>
<li><strong>Grayscale conversion</strong>: Reduced data dimensionality while preserving tag contrast, improving speed by ~30%</li>
<li><strong>Optimal camera matrix</strong>: Preserved field of view while correcting lens distortions</li>
</ul>
</li>
</ol>
<h4>AprilTag Detection Implementation</h4>
<p>We implemented AprilTag detection using the <code>dt_apriltags</code> library, which allows us to detect the tag36h11 family of AprilTags:</p>
<pre><code class="language-python">from dt_apriltags import Detector

# Initialize AprilTag detector
detector = dt_apriltags.Detector(families=&quot;tag36h11&quot;)

# Detect AprilTags in the image
results = detector.detect(gray)
</code></pre>
<p>Our implementation identifies three specific AprilTag IDs, defined in our <code>apriltags.py</code> file:</p>
<pre><code class="language-python">class AprilTag(Enum):
    UALBERTA = 201
    INTERSECTIONT = 133
    STOP = 21
</code></pre>
<p>We implemented our detection with a rate of 10Hz, which provided a good balance between:</p>
<ol>
<li>Avoid excessive overhead and overloading the Duckiebot&#x27;s processor.</li>
<li>Sufficient frequency to ensure consistent detection of tags even at moderate speeds.</li>
</ol>
<p>For each detected tag, we:</p>
<ol>
<li>Identified the specific tag type based on its ID.</li>
<li>Published the state to a ROS topic for other nodes to react to.</li>
</ol>
<p>Our 10Hz detection rate was chosen based on:</p>
<ul>
<li><strong>Robot speed</strong>: At about 0.3 m/s, this checks for tags every ~3cm, ensuring reliable detection</li>
<li><strong>Experimental testing</strong>: 5Hz missed tags at full speed, &gt;20Hz caused lag in other functions</li>
</ul>
<h4>LED Control Based on AprilTag Detection</h4>
<p>We implemented LED color changes based on the detected AprilTag using a dedicated <code>LEDControl</code> node that subscribes to the robot&#x27;s state:</p>
<pre><code class="language-python">class LEDControl(DTROS):
    def __init__(self, node_name):
        # ...
        self.states = [
            State(message_name=&quot;No tag detected&quot;, colorPattern=ColorPattern(frontLeft=Colors.White, frontRight=Colors.White, backLeft=Colors.White, backRight=Colors.White)),
            State(message_name=&quot;INTERSECTIONT tag detected&quot;, colorPattern=ColorPattern(frontLeft=Colors.Blue, frontRight=Colors.Blue, backLeft=Colors.Blue, backRight=Colors.Blue)),
            State(message_name=&quot;STOP tag detected&quot;, colorPattern=ColorPattern(frontLeft=Colors.Red, frontRight=Colors.Red, backLeft=Colors.Red, backRight=Colors.Red)),
            State(message_name=&quot;UALBERTA tag detected&quot;, colorPattern=ColorPattern(frontLeft=Colors.Green, frontRight=Colors.Green, backLeft=Colors.Green, backRight=Colors.Green)),
            # Additional states for other behaviors
        ]
</code></pre>
<p>This implementation sets:</p>
<ul>
<li><strong>Red</strong>: When a Stop Sign tag (ID: 21) is detected</li>
<li><strong>Blue</strong>: When a T-Intersection tag (ID: 133) is detected</li>
<li><strong>Green</strong>: When a UofA tag (ID: 201) is detected</li>
<li><strong>White</strong>: Default state (no detection)</li>
</ul>
<h4>Intersection Behavior</h4>
<p>When approaching an intersection, our Duckiebot:</p>
<ol>
<li>
<p>Detects red intersection lines using a color detection approach similar to Exercise 3</p>
</li>
<li>
<p>Stops for a duration based on the last seen AprilTag, implemented in the <code>StopBehavior</code> class:</p>
<pre><code class="language-python">class StopBehavior():
    def __init__(self, red_detection_magnitude, stop_time=3):
        self.detection_magnitude=red_detection_magnitude
        self.stop_time = stop_time

    def execute(self, dtros):
        # Find red line to stop
        # ...

        # Stop for the appropriate time based on tag
        message = WheelsCmdStamped(vel_left=0, vel_right=0)
        dtros._publisher.publish(message)
        rospy.sleep(self.stop_time)
</code></pre>
<p>With stop times of:</p>
<ul>
<li>Stop Sign (ID: 21): 3 seconds</li>
<li>T-Intersection (ID: 133): 2 seconds</li>
<li>UofA Tag (ID: 201): 1 second</li>
<li>No detection (default): 0.5 seconds</li>
</ul>
</li>
</ol>
<p>We integrated AprilTag detection with intersection behavior through:</p>
<ul>
<li>Parallel processing with separate nodes for detection and intersection monitoring</li>
<li>A &quot;tag memory&quot; system that remembers the last detected tag:<!-- -->
<pre><code class="language-python">def tag_detection_callback(self, msg):
    self.last_detected_tag = msg.data
    self.last_detection_time = rospy.get_time()
</code></pre>
</li>
<li>State-based decision making to determine stop time at intersections</li>
</ul>
<video src="/cmput412-akhadeli/images/ex4/apriltag_screencast.webm" controls=""></video>
<p>This screencast demonstrates our AprilTag detection system in action. In the rqt_image_view, we see successful identification of all three types of tags: Stop Sign (ID: 21), T-Intersection (ID: 133), and UofA (ID: 201). Each tag is highlighted with a colored bounding box and labeled with its ID number. We also observe that detections can be made at different angles, and multiple tags can be detected at the same time.</p>
<hr/>
<video src="/cmput412-akhadeli/images/ex4/apriltag_led.mp4" controls=""></video>
<p>This video showcases our Duckiebot responding to different AprilTags by changing its LED lights accordingly. When it detects a Stop Sign tag (ID: 21), the LEDs turn red. For T-Intersection tags (ID: 133), they turn blue. When seeing a UofA tag (ID: 201), they turn green. And when no tag is in view, the LEDs remain white. We can also see that the robot stops at the intersection for the respective amount of time.</p>
<hr/>
<h3>Part II - PeDuckstrian Crosswalks</h3>
<h4>Crosswalk Detection</h4>
<ol>
<li>
<p><strong>Detecting Blue Crosswalk Lines</strong>:</p>
<pre><code class="language-python"># Blue tape detection for crosswalks
lower_blue = np.array([100, 150, 0], dtype=np.uint8)
upper_blue = np.array([140, 255, 255], dtype=np.uint8)
blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)

# Apply dilation to improve detection
kernel = np.ones((5, 5), &quot;uint8&quot;)
blue_mask = cv2.dilate(blue_mask, kernel)

# Find contours for blue lines
contours, _ = cv2.findContours(blue_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
</code></pre>
</li>
<li>
<p><strong>Handling Double Blue Lines</strong>:</p>
<ul>
<li>We implemented a task-based approach to handle crosswalks with the <code>PDCrossWalkMain</code> class</li>
<li>The robot stops at the first blue line using our <code>FindCrossWalk</code> task, which detects blue lines with a magnitude threshold of 3000 pixels</li>
<li>We determine when to stop by calculating the distance from the bottom of the frame to the closest blue line pixels:</li>
</ul>
<pre><code class="language-python">def getCenterDistanceFromBottom(self, mask):
    y_coords, x_coords = np.where(mask &gt; 0)
    active_pixel_count = len(x_coords)

    if active_pixel_count &lt; self.detection_magnitude:
        return math.inf  # Not enough active pixels

    # Compute distance from the bottom of the image
    img_height = mask.shape[0]
    distance_from_bottom = img_height - max(y_coords)

    return distance_from_bottom
</code></pre>
<ul>
<li>After stopping and checking for peDuckstrians, we use the <code>DrivePastCrossWalk</code> task to proceed through the crosswalk, defining &quot;past the crosswalk&quot; as when the blue detection magnitude falls below our threshold:</li>
</ul>
<pre><code class="language-python">def isDetectingCrossWalk(self, homography_mask):
    active_pixel_count = np.count_nonzero(homography_mask &gt; 0)

    if active_pixel_count &gt; self.detection_magnitude:
        return True  # Crosswalk detected
    return False  # No crosswalk detected
</code></pre>
</li>
</ol>
<p>We handled double blue lines through a state-based approach:</p>
<ul>
<li>Used a state machine to track progress (looking for first line → waiting/processing → driving past both lines)</li>
<li>Implemented a &quot;blue line memory&quot; flag after stopping at the first line:<!-- -->
<pre><code class="language-python">if self.has_stopped_at_crosswalk:
    drive_forward()  # Already stopped, continue through second line
else:
    stop()
    self.has_stopped_at_crosswalk = True  # First line encountered
</code></pre>
</li>
<li>Used distance-based identification to determine the first line</li>
<li>Reset the state after passing both lines, preparing for the next crosswalk</li>
</ul>
<h4>PeDuckstrian Detection</h4>
<p>To detect peDuckstrians crossing at crosswalks:</p>
<ol>
<li>
<p>Our implementation looks for yellow objects in the homography-transformed image:</p>
<pre><code class="language-python">def isPeduckstriansVisible(self, mask):
    _, x_coords = np.where(mask &gt; 0)

    if len(x_coords) == 0:
        return False

    x_variance = np.var(x_coords)
    magnitude = len(x_coords)

    if (magnitude &gt;= self.detection_magnitude):
        if(x_variance &gt;= self.detection_variance):
            print(&quot;Peduckstrians detected&quot;)
            return True
    return False
</code></pre>
</li>
<li>
<p><strong>Crosswalk Behavior</strong>:</p>
<ul>
<li>We use a sequence of tasks to handle the crosswalk behavior:</li>
</ul>
<pre><code class="language-python">tasks = [
    FindCrossWalk(detection_magnitude=3000, stopping_distance_in_pixels=5),
    WaitForPeduckstrians(detection_magnitude=2000, detection_variance=5000),
    Stop(stop_time=1),
    DrivePastCrossWalk(detection_magnitude=3000),
    # ... repeated for multiple crosswalks
]
</code></pre>
<ul>
<li>By sequencing the tasks, we exit the find crosswalk task to then detect and wait for peDuckstrians to clear the crosswalk</li>
<li>The <code>WaitForPeduckstrians</code> task checks if peDuckstrians are present by analyzing both the magnitude and variance in the yellow mask</li>
<li>If the magnitude and variance thresholds are met, the robot waits until peDuckstrians have cleared</li>
<li>After peDuckstrians have cleared or if none are detected, the robot stops for 1 second, then proceeds through the crosswalk</li>
</ul>
</li>
</ol>
<p>Detection method:</p>
<ul>
<li><strong>Color filtering</strong>: Used HSV thresholds to isolate yellow objects</li>
<li><strong>Magnitude threshold</strong>: Required at least 2000 yellow pixels to indicate a significant object</li>
<li><strong>Variance analysis</strong>: Measured the spread of yellow pixels (variance &gt; 5000) to distinguish between:<!-- -->
<ul>
<li>PeDuckstrian strips (high variance, spread across road)</li>
<li>Other yellow objects (typically lower variance)</li>
</ul>
</li>
</ul>
<p>Potential failure scenarios include:</p>
<ul>
<li>Extreme lighting affecting color detection</li>
<li>Partial visibility of strips at frame edges</li>
<li>Other yellow objects causing false positives</li>
<li>Occlusion by other objects</li>
</ul>
<p>Experimental testing of magnitude and variance thresholds helped us select values which mitigated these failure scenarios.</p>
<video src="/cmput412-akhadeli/images/ex4/crosswalk_video.mp4" controls=""></video>
<p>This video demonstrates our Duckiebot&#x27;s behavior at crosswalks. Initially, the Duckiebot approaches an empty crosswalk, stops for 1 second at the first blue line, and then proceeds. Next, when it encounters a crosswalk with peDuckstrians, it waits until they&#x27;ve cleared before continuing. The Duckiebot successfully distinguishes between the two cases, stopping only at the first blue line and waiting appropriately when peDuckstrians are present.</p>
<hr/>
<h3>Part III - Safe Navigation</h3>
<h4>Broken-Down Duckiebot Detection</h4>
<ol>
<li>
<p><strong>Detection Approach</strong>:</p>
<pre><code class="language-python">def blue_detection_callback(self, msg):
    # msg is already a mask
    mask_blue = self._bridge.compressed_imgmsg_to_cv2(msg)

    # Find the coordinates of active (non-zero) pixels
    y_coords, x_coords = np.where(mask_blue &gt; 0)  # y, x positions of active pixels

    if len(x_coords) == 0:
        return

    # Calculate the variance of the x coordinates
    x_variance = np.var(x_coords)

    # Calculate the magnitude (number of active blue pixels)
    magnitude = len(x_coords)

    if (magnitude &gt;= self.duckie_detection_sensitivity):
        if(x_variance &lt;= self.duckie_detection_distance):
            self._duckie_detected = True
            self._duckie_detected_time_stamp = time.time()
            return
</code></pre>
</li>
<li>
<p><strong>Safe Distance Stopping</strong>:</p>
<ul>
<li>We detect the broken-down Duckiebot using blue color detection.</li>
<li>Our approach uses two key parameters:<!-- -->
<ul>
<li><code>duckie_detection_sensitivity</code>: Threshold for the number of blue pixels (set to 2000)</li>
<li><code>duckie_detection_distance</code>: Maximum variance of blue pixels (set to 30000)</li>
</ul>
</li>
<li>When blue pixels with high magnitude and low variance are detected, we consider it to be a broken-down Duckiebot</li>
<li>The robot automatically stops when it detects the broken-down Duckiebot and sets a timestamp for lane change planning.</li>
</ul>
</li>
</ol>
<p>Our method for detecting broken-down Duckiebots used a color-based approach with geometric constraints:</p>
<ul>
<li>Targeted the blue shell as the most distinctive feature</li>
<li>Used dual parameters for reliable detection:<!-- -->
<ul>
<li>Magnitude threshold (2000 pixels) to ensure sufficient size</li>
<li>Variance constraint (below 30000) to verify compact clustering</li>
</ul>
</li>
<li>Required persistent detection across multiple frames to reduce false positives</li>
</ul>
<p>We tried several other approaches before settling on this method:</p>
<ul>
<li>Tracking based detection using Lucas-Kanade optical flow<!-- -->
<ul>
<li>We found it hard to implement and time consuming to get working</li>
</ul>
</li>
</ul>
<p>Our final approach provided the best balance of reliability and computational efficiency.</p>
<h4>Navigation Around Obstacles</h4>
<p>We implemented a task-based approach for safe navigation around broken-down Duckiebots with three main tasks:</p>
<ol>
<li>
<p><strong>Vehicle Avoidance Detection</strong>:</p>
<pre><code class="language-python">class VehicleAvoidance(VehicleAvoidanceTask):
    def runTask(self, dtros):
        print(&quot;VehicleAvoidance&quot;)
        rate = rospy.Rate(10)

        while not rospy.is_shutdown():
            correctionUpdate = dtros.getUpdate()

            if(dtros._duckie_detected_time_stamp is not None and (time.time() - dtros._duckie_detected_time_stamp) &lt; dtros.lane_correction_delay):
                print(&quot;Duckie Detected&quot;)
                break

            # Continue driving until detection
            # ...
</code></pre>
</li>
<li>
<p><strong>Stop and Assess</strong>:</p>
<pre><code class="language-python">class Stop(VehicleAvoidanceTask):
    def __init__(self, stop_time=3):
        super().__init__()
        self._stop_time = stop_time

    def runTask(self, dtros):
        print(&quot;Stopping&quot;)
        stop = WheelsCmdStamped(vel_left=0, vel_right=0)
        dtros._publisher.publish(stop)
        time.sleep(self._stop_time)
</code></pre>
</li>
<li>
<p><strong>Lane Change and Return</strong>:</p>
<pre><code class="language-python">class SwitchToLeftLane(VehicleAvoidanceTask):
    def runTask(self, dtros):
        print(&quot;SwitchToLeftLane&quot;)
        rate = rospy.Rate(10)

        while not rospy.is_shutdown():
            correctionUpdate = dtros.getUpdate()

            if dtros._duckie_detected_time_stamp is not None and (time.time() - dtros._duckie_detected_time_stamp) &gt;= dtros.lane_correction_delay:
                break

            # Continue in left lane until timeout
            # ...
</code></pre>
</li>
</ol>
<p>After detecting the broken-down Duckiebot, the robot swaps its lane tracking targets:</p>
<pre><code class="language-python">if self._duckie_detected and self._duckie_detected_time_stamp is not None and (time.time() - self._duckie_detected_time_stamp) &lt; self.lane_correction_delay:
    yellow_error = self.compute_error(mask=mask_yellow, target_x=489)
    white_error = self.compute_error(mask=mask_white, target_x=100)
else:
    yellow_error = self.compute_error(mask=mask_yellow, target_x=100)
    white_error = self.compute_error(mask=mask_white, target_x=489)
</code></pre>
<p>This effectively inverts the lane-following algorithm, causing the robot to switch to the left lane while maintaining its ability to follow the lane markers.</p>
<p>Our maneuvering approach leveraged our existing lane-following system with key additions:</p>
<ul>
<li><strong>Target position inversion</strong>: Dynamically switched target positions for yellow and white lane markers<!-- -->
<pre><code class="language-python"># Normal vs. overtaking lane targets
yellow_error = self.compute_error(mask=mask_yellow, target_x=100)  # Normal: yellow on left
yellow_error = self.compute_error(mask=mask_yellow, target_x=489)  # Overtaking: yellow on right
</code></pre>
</li>
<li><strong>Sequential execution</strong>: Three-phase process (detection → assessment → avoidance)</li>
<li><strong>PID controller reuse</strong>: Same controller with different targets for smooth transitions</li>
<li>This approach ensured stable lane changes while leveraging our well-tuned lane following system</li>
</ul>
<video src="/cmput412-akhadeli/images/ex4/safe_navigation.mp4" controls=""></video>
<p>This video showcases our Duckiebot&#x27;s obstacle avoidance capabilities. It begins with the robot driving straight until it detects a broken-down Duckiebot in its path. Upon detection, our robot stops at a safe distance and pauses for 3 seconds. Then, it executes the lane change maneuver, navigating into the opposing lane to safely pass the obstacle. Once it has cleared the broken-down bot, our Duckiebot automatically returns to the original right lane and continues driving.</p>
<hr/>
<h3>Reflection</h3>
<p>We found this exercise very insightful, as we got the opportunity to work with AprilTag detection and advanced navigation behaviors. The integration of multiple systems presented unique challenges that required careful coordination of perception and control components.</p>
<h3>Code</h3>
<p>Find the code using the link
<a href="https://github.com/akhadeli/cmput412-akhadeli-code/tree/main/exercise-4">Code</a></p>
<h3>References</h3>
<ul>
<li><a href="https://wiki.ros.org/ROS/Tutorials">ROS Documentation</a></li>
<li><a href="https://docs.duckietown.com/daffy/devmanual-software/beginner">Duckiebot Documentation</a></li>
<li><a href="https://medium.com/@nahmed3536/wheel-odometry-model-for-differential-drive-robotics-91b85a012299">Odometry Medium Article</a></li>
<li><a href="https://david010.medium.com/lane-tracking-via-computer-vision-2acb4c7c1c22">Lane Tracking Article</a></li>
<li><a href="https://docs.opencv.org/">OpenCV Documentation</a></li>
<li><a href="https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/">GeeksForGeeks Article</a></li>
<li><a href="https://en.wikipedia.org/wiki/PID_controller">PID Controller Wikipedia</a></li>
<li><a href="https://www.youtube.com/watch?v=y3K6FUgrgXw">PID Controllers in Unity</a></li>
<li><a href="https://en.wikipedia.org/wiki/HSL_and_HSV">HSV Color Space Wikipedia</a></li>
<li><a href="https://github.com/duckietown/lib-dt-apriltags">AprilTag Library Documentation</a></li>
<li><a href="https://april.eecs.umich.edu/software/apriltag">April Robotics Laboratory</a></li>
</ul></div></main></main></div><script src="/cmput412-akhadeli/_next/static/chunks/webpack-9df4f922c7104209.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[2034,[\"173\",\"static/chunks/173-7be02dd55462ff0c.js\",\"678\",\"static/chunks/678-9555f6ed956a8d40.js\",\"177\",\"static/chunks/app/layout-ebf74d6892f2c1b6.js\"],\"SidebarProvider\"]\n3:I[1543,[\"173\",\"static/chunks/173-7be02dd55462ff0c.js\",\"678\",\"static/chunks/678-9555f6ed956a8d40.js\",\"177\",\"static/chunks/app/layout-ebf74d6892f2c1b6.js\"],\"AppSidebar\"]\n4:I[2034,[\"173\",\"static/chunks/173-7be02dd55462ff0c.js\",\"678\",\"static/chunks/678-9555f6ed956a8d40.js\",\"177\",\"static/chunks/app/layout-ebf74d6892f2c1b6.js\"],\"SidebarTrigger\"]\n5:I[5244,[],\"\"]\n6:I[3866,[],\"\"]\n8:I[6213,[],\"OutletBoundary\"]\na:I[6213,[],\"MetadataBoundary\"]\nc:I[6213,[],\"ViewportBoundary\"]\ne:I[4835,[],\"\"]\n:HL[\"/cmput412-akhadeli/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/cmput412-akhadeli/_next/static/css/f8c155f11af27ff1.css\",\"style\"]\n:HL[\"/cmput412-akhadeli/_next/static/css/e23141aac3fba32d.css\",\"style\"]\n7:T456,prose prose-headings:mt-8 prose-headings:font-semibold prose-headings:text-black prose-h1:text-5xl prose-h2:text-4xl prose-h3:text-3xl prose-h4:text-2xl prose-h5:text-xl prose-h6:text-lg dark:prose-headings:text-white prose-md prose-p:text-lg prose-p:leading-relaxed prose-p:text-gray-800 dark:prose-p:text-gray-100 prose-table:w-full prose-table:border-collapse prose-table:border-gray-300 prose-table:dark:border-gray-600 prose-table:text-left prose-table:text-sm prose-table:text-gray-800 dark:prose-table:text-gray-100 prose-ul:text-gray-800 dark:prose-ul:text-gray-100 prose-li:text-gray-800 dark:prose-li:text-gray-100 prose-strong:text-gray-800 prose-strong:font-bold dark:prose-strong:text-gray-100 prose-img:w-full prose-img:rounded-lg prose-img:border prose-img:border-border prose-img:dark:border-border prose-video:w-full prose-video:rounded-lg prose-video:border prose-video:border-border prose-video:dark:border-border prose-iframe:w-full prose-iframe:rounded-lg prose-iframe:border prose-iframe:border-border prose-iframe:dark:border-border prose-span:text-gray-800 prose-span:dark:"])</script><script>self.__next_f.push([1,"text-gray-100"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"IyTIDrFCTj8nxwzP0krqc\",\"p\":\"/cmput412-akhadeli\",\"c\":[\"\",\"exercises\",\"ex4\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"exercises\",{\"children\":[\"ex4\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cmput412-akhadeli/_next/static/css/f8c155f11af27ff1.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"__className_23f766 antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{}],[\"$\",\"$L4\",null,{\"className\":\"ml-3 mt-3\"}],[\"$\",\"main\",null,{\"className\":\"flex-1 overflow-auto p-8 pt-16\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]}]}]]}],{\"children\":[\"exercises\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"exercises\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"ex4\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"exercises\",\"children\",\"ex4\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"main\",null,{\"className\":\"mx-auto max-w-3xl\",\"children\":[\"$\",\"div\",null,{\"className\":\"$7\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"CMPUT 412/503 - Exercise 4 Report\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Abdullah Khadeli and Ryan Rom\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Part I - AprilTag Detection\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Camera Calibration and Image Processing\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Camera Undistortion\"}],\": We used our camera calibration file to undistort the images captured by the Duckiebot's camera, using the same approach as in Exercise 3:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"# Subscribing to camera topic\\nself.sub = rospy.Subscriber(self._camera_topic, CompressedImage, self.callback)\\n\\n# In the callback function\\nimage = self._bridge.compressed_imgmsg_to_cv2(msg)\\nundistorted_image = cv2.undistort(image, camera_matrix, dist_coeffs)\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We used the following camera intrinsics for undistortion:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"# Camera intrinsics\\nK = np.array([[310.0149584021843, 0.0, 307.7177249518777],\\n            [0.0, 309.29643750324607, 229.191787718834],\\n            [0.0, 0.0, 1.0]], dtype=np.float32)\\n\\nD = np.array([-0.2683225140828933, 0.049595473114203516,\\n            0.0003617920649662741, 0.006030049583437601, 0.0], dtype=np.float32)\\n\\n# Compute optimal camera matrix to minimize black areas after undistortion\\nnew_K, roi = cv2.getOptimalNewCameraMatrix(K, D, (img_width, img_height), 1, (img_width, img_height))\\n\\n# Undistort image\\nundistorted = cv2.undistort(image, K, D, None, new_K)\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Image Preprocessing\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"We used the optimal camera matrix to minimize black areas after undistortion\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Cropped the image based on ROI (Region of Interest) after undistortion\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Converted the image to grayscale for processing with the AprilTag detector\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"For better detection, we focused on the right half of the undistorted image:\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"def publish_undistort_grayscale(self, undistort):\\n    image = undistort\\n    # Convert to grayscale and store it in grayscale_image\\n    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\\n    # Get image dimensions\\n    height, width = grayscale_image.shape\\n\\n    # Crop the right half of grayscale_image\\n    grayscale_image = grayscale_image[:, width // 2:]\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our preprocessing improved detection by:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cropping the right half\"}],\": Most tags were on the right side of the road, reducing computational load and false positives\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Grayscale conversion\"}],\": Reduced data dimensionality while preserving tag contrast, improving speed by ~30%\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Optimal camera matrix\"}],\": Preserved field of view while correcting lens distortions\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"AprilTag Detection Implementation\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We implemented AprilTag detection using the \",[\"$\",\"code\",null,{\"children\":\"dt_apriltags\"}],\" library, which allows us to detect the tag36h11 family of AprilTags:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"from dt_apriltags import Detector\\n\\n# Initialize AprilTag detector\\ndetector = dt_apriltags.Detector(families=\\\"tag36h11\\\")\\n\\n# Detect AprilTags in the image\\nresults = detector.detect(gray)\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Our implementation identifies three specific AprilTag IDs, defined in our \",[\"$\",\"code\",null,{\"children\":\"apriltags.py\"}],\" file:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class AprilTag(Enum):\\n    UALBERTA = 201\\n    INTERSECTIONT = 133\\n    STOP = 21\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We implemented our detection with a rate of 10Hz, which provided a good balance between:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Avoid excessive overhead and overloading the Duckiebot's processor.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Sufficient frequency to ensure consistent detection of tags even at moderate speeds.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"For each detected tag, we:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Identified the specific tag type based on its ID.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Published the state to a ROS topic for other nodes to react to.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our 10Hz detection rate was chosen based on:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Robot speed\"}],\": At about 0.3 m/s, this checks for tags every ~3cm, ensuring reliable detection\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Experimental testing\"}],\": 5Hz missed tags at full speed, \u003e20Hz caused lag in other functions\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"LED Control Based on AprilTag Detection\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We implemented LED color changes based on the detected AprilTag using a dedicated \",[\"$\",\"code\",null,{\"children\":\"LEDControl\"}],\" node that subscribes to the robot's state:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class LEDControl(DTROS):\\n    def __init__(self, node_name):\\n        # ...\\n        self.states = [\\n            State(message_name=\\\"No tag detected\\\", colorPattern=ColorPattern(frontLeft=Colors.White, frontRight=Colors.White, backLeft=Colors.White, backRight=Colors.White)),\\n            State(message_name=\\\"INTERSECTIONT tag detected\\\", colorPattern=ColorPattern(frontLeft=Colors.Blue, frontRight=Colors.Blue, backLeft=Colors.Blue, backRight=Colors.Blue)),\\n            State(message_name=\\\"STOP tag detected\\\", colorPattern=ColorPattern(frontLeft=Colors.Red, frontRight=Colors.Red, backLeft=Colors.Red, backRight=Colors.Red)),\\n            State(message_name=\\\"UALBERTA tag detected\\\", colorPattern=ColorPattern(frontLeft=Colors.Green, frontRight=Colors.Green, backLeft=Colors.Green, backRight=Colors.Green)),\\n            # Additional states for other behaviors\\n        ]\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This implementation sets:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Red\"}],\": When a Stop Sign tag (ID: 21) is detected\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Blue\"}],\": When a T-Intersection tag (ID: 133) is detected\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Green\"}],\": When a UofA tag (ID: 201) is detected\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"White\"}],\": Default state (no detection)\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Intersection Behavior\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"When approaching an intersection, our Duckiebot:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Detects red intersection lines using a color detection approach similar to Exercise 3\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Stops for a duration based on the last seen AprilTag, implemented in the \",[\"$\",\"code\",null,{\"children\":\"StopBehavior\"}],\" class:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class StopBehavior():\\n    def __init__(self, red_detection_magnitude, stop_time=3):\\n        self.detection_magnitude=red_detection_magnitude\\n        self.stop_time = stop_time\\n\\n    def execute(self, dtros):\\n        # Find red line to stop\\n        # ...\\n\\n        # Stop for the appropriate time based on tag\\n        message = WheelsCmdStamped(vel_left=0, vel_right=0)\\n        dtros._publisher.publish(message)\\n        rospy.sleep(self.stop_time)\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"With stop times of:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Stop Sign (ID: 21): 3 seconds\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"T-Intersection (ID: 133): 2 seconds\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"UofA Tag (ID: 201): 1 second\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"No detection (default): 0.5 seconds\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We integrated AprilTag detection with intersection behavior through:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Parallel processing with separate nodes for detection and intersection monitoring\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"A \\\"tag memory\\\" system that remembers the last detected tag:\",\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"def tag_detection_callback(self, msg):\\n    self.last_detected_tag = msg.data\\n    self.last_detection_time = rospy.get_time()\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"State-based decision making to determine stop time at intersections\"}],\"\\n\"]}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex4/apriltag_screencast.webm\",\"controls\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This screencast demonstrates our AprilTag detection system in action. In the rqt_image_view, we see successful identification of all three types of tags: Stop Sign (ID: 21), T-Intersection (ID: 133), and UofA (ID: 201). Each tag is highlighted with a colored bounding box and labeled with its ID number. We also observe that detections can be made at different angles, and multiple tags can be detected at the same time.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex4/apriltag_led.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This video showcases our Duckiebot responding to different AprilTags by changing its LED lights accordingly. When it detects a Stop Sign tag (ID: 21), the LEDs turn red. For T-Intersection tags (ID: 133), they turn blue. When seeing a UofA tag (ID: 201), they turn green. And when no tag is in view, the LEDs remain white. We can also see that the robot stops at the intersection for the respective amount of time.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Part II - PeDuckstrian Crosswalks\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Crosswalk Detection\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Detecting Blue Crosswalk Lines\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"# Blue tape detection for crosswalks\\nlower_blue = np.array([100, 150, 0], dtype=np.uint8)\\nupper_blue = np.array([140, 255, 255], dtype=np.uint8)\\nblue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\\n\\n# Apply dilation to improve detection\\nkernel = np.ones((5, 5), \\\"uint8\\\")\\nblue_mask = cv2.dilate(blue_mask, kernel)\\n\\n# Find contours for blue lines\\ncontours, _ = cv2.findContours(blue_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Handling Double Blue Lines\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"We implemented a task-based approach to handle crosswalks with the \",[\"$\",\"code\",null,{\"children\":\"PDCrossWalkMain\"}],\" class\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The robot stops at the first blue line using our \",[\"$\",\"code\",null,{\"children\":\"FindCrossWalk\"}],\" task, which detects blue lines with a magnitude threshold of 3000 pixels\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"We determine when to stop by calculating the distance from the bottom of the frame to the closest blue line pixels:\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"def getCenterDistanceFromBottom(self, mask):\\n    y_coords, x_coords = np.where(mask \u003e 0)\\n    active_pixel_count = len(x_coords)\\n\\n    if active_pixel_count \u003c self.detection_magnitude:\\n        return math.inf  # Not enough active pixels\\n\\n    # Compute distance from the bottom of the image\\n    img_height = mask.shape[0]\\n    distance_from_bottom = img_height - max(y_coords)\\n\\n    return distance_from_bottom\\n\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"After stopping and checking for peDuckstrians, we use the \",[\"$\",\"code\",null,{\"children\":\"DrivePastCrossWalk\"}],\" task to proceed through the crosswalk, defining \\\"past the crosswalk\\\" as when the blue detection magnitude falls below our threshold:\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"def isDetectingCrossWalk(self, homography_mask):\\n    active_pixel_count = np.count_nonzero(homography_mask \u003e 0)\\n\\n    if active_pixel_count \u003e self.detection_magnitude:\\n        return True  # Crosswalk detected\\n    return False  # No crosswalk detected\\n\"}]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We handled double blue lines through a state-based approach:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Used a state machine to track progress (looking for first line → waiting/processing → driving past both lines)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Implemented a \\\"blue line memory\\\" flag after stopping at the first line:\",\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"if self.has_stopped_at_crosswalk:\\n    drive_forward()  # Already stopped, continue through second line\\nelse:\\n    stop()\\n    self.has_stopped_at_crosswalk = True  # First line encountered\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Used distance-based identification to determine the first line\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Reset the state after passing both lines, preparing for the next crosswalk\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"PeDuckstrian Detection\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"To detect peDuckstrians crossing at crosswalks:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our implementation looks for yellow objects in the homography-transformed image:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"def isPeduckstriansVisible(self, mask):\\n    _, x_coords = np.where(mask \u003e 0)\\n\\n    if len(x_coords) == 0:\\n        return False\\n\\n    x_variance = np.var(x_coords)\\n    magnitude = len(x_coords)\\n\\n    if (magnitude \u003e= self.detection_magnitude):\\n        if(x_variance \u003e= self.detection_variance):\\n            print(\\\"Peduckstrians detected\\\")\\n            return True\\n    return False\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Crosswalk Behavior\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"We use a sequence of tasks to handle the crosswalk behavior:\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"tasks = [\\n    FindCrossWalk(detection_magnitude=3000, stopping_distance_in_pixels=5),\\n    WaitForPeduckstrians(detection_magnitude=2000, detection_variance=5000),\\n    Stop(stop_time=1),\\n    DrivePastCrossWalk(detection_magnitude=3000),\\n    # ... repeated for multiple crosswalks\\n]\\n\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"By sequencing the tasks, we exit the find crosswalk task to then detect and wait for peDuckstrians to clear the crosswalk\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"WaitForPeduckstrians\"}],\" task checks if peDuckstrians are present by analyzing both the magnitude and variance in the yellow mask\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"If the magnitude and variance thresholds are met, the robot waits until peDuckstrians have cleared\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"After peDuckstrians have cleared or if none are detected, the robot stops for 1 second, then proceeds through the crosswalk\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Detection method:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Color filtering\"}],\": Used HSV thresholds to isolate yellow objects\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Magnitude threshold\"}],\": Required at least 2000 yellow pixels to indicate a significant object\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Variance analysis\"}],\": Measured the spread of yellow pixels (variance \u003e 5000) to distinguish between:\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"PeDuckstrian strips (high variance, spread across road)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Other yellow objects (typically lower variance)\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Potential failure scenarios include:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Extreme lighting affecting color detection\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Partial visibility of strips at frame edges\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Other yellow objects causing false positives\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Occlusion by other objects\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Experimental testing of magnitude and variance thresholds helped us select values which mitigated these failure scenarios.\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex4/crosswalk_video.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This video demonstrates our Duckiebot's behavior at crosswalks. Initially, the Duckiebot approaches an empty crosswalk, stops for 1 second at the first blue line, and then proceeds. Next, when it encounters a crosswalk with peDuckstrians, it waits until they've cleared before continuing. The Duckiebot successfully distinguishes between the two cases, stopping only at the first blue line and waiting appropriately when peDuckstrians are present.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Part III - Safe Navigation\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Broken-Down Duckiebot Detection\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Detection Approach\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"def blue_detection_callback(self, msg):\\n    # msg is already a mask\\n    mask_blue = self._bridge.compressed_imgmsg_to_cv2(msg)\\n\\n    # Find the coordinates of active (non-zero) pixels\\n    y_coords, x_coords = np.where(mask_blue \u003e 0)  # y, x positions of active pixels\\n\\n    if len(x_coords) == 0:\\n        return\\n\\n    # Calculate the variance of the x coordinates\\n    x_variance = np.var(x_coords)\\n\\n    # Calculate the magnitude (number of active blue pixels)\\n    magnitude = len(x_coords)\\n\\n    if (magnitude \u003e= self.duckie_detection_sensitivity):\\n        if(x_variance \u003c= self.duckie_detection_distance):\\n            self._duckie_detected = True\\n            self._duckie_detected_time_stamp = time.time()\\n            return\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Safe Distance Stopping\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"We detect the broken-down Duckiebot using blue color detection.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Our approach uses two key parameters:\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"duckie_detection_sensitivity\"}],\": Threshold for the number of blue pixels (set to 2000)\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"duckie_detection_distance\"}],\": Maximum variance of blue pixels (set to 30000)\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"When blue pixels with high magnitude and low variance are detected, we consider it to be a broken-down Duckiebot\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The robot automatically stops when it detects the broken-down Duckiebot and sets a timestamp for lane change planning.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our method for detecting broken-down Duckiebots used a color-based approach with geometric constraints:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Targeted the blue shell as the most distinctive feature\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Used dual parameters for reliable detection:\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Magnitude threshold (2000 pixels) to ensure sufficient size\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Variance constraint (below 30000) to verify compact clustering\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Required persistent detection across multiple frames to reduce false positives\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We tried several other approaches before settling on this method:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Tracking based detection using Lucas-Kanade optical flow\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"We found it hard to implement and time consuming to get working\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our final approach provided the best balance of reliability and computational efficiency.\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Navigation Around Obstacles\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We implemented a task-based approach for safe navigation around broken-down Duckiebots with three main tasks:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Vehicle Avoidance Detection\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class VehicleAvoidance(VehicleAvoidanceTask):\\n    def runTask(self, dtros):\\n        print(\\\"VehicleAvoidance\\\")\\n        rate = rospy.Rate(10)\\n\\n        while not rospy.is_shutdown():\\n            correctionUpdate = dtros.getUpdate()\\n\\n            if(dtros._duckie_detected_time_stamp is not None and (time.time() - dtros._duckie_detected_time_stamp) \u003c dtros.lane_correction_delay):\\n                print(\\\"Duckie Detected\\\")\\n                break\\n\\n            # Continue driving until detection\\n            # ...\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Stop and Assess\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class Stop(VehicleAvoidanceTask):\\n    def __init__(self, stop_time=3):\\n        super().__init__()\\n        self._stop_time = stop_time\\n\\n    def runTask(self, dtros):\\n        print(\\\"Stopping\\\")\\n        stop = WheelsCmdStamped(vel_left=0, vel_right=0)\\n        dtros._publisher.publish(stop)\\n        time.sleep(self._stop_time)\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Lane Change and Return\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"class SwitchToLeftLane(VehicleAvoidanceTask):\\n    def runTask(self, dtros):\\n        print(\\\"SwitchToLeftLane\\\")\\n        rate = rospy.Rate(10)\\n\\n        while not rospy.is_shutdown():\\n            correctionUpdate = dtros.getUpdate()\\n\\n            if dtros._duckie_detected_time_stamp is not None and (time.time() - dtros._duckie_detected_time_stamp) \u003e= dtros.lane_correction_delay:\\n                break\\n\\n            # Continue in left lane until timeout\\n            # ...\\n\"}]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"After detecting the broken-down Duckiebot, the robot swaps its lane tracking targets:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"if self._duckie_detected and self._duckie_detected_time_stamp is not None and (time.time() - self._duckie_detected_time_stamp) \u003c self.lane_correction_delay:\\n    yellow_error = self.compute_error(mask=mask_yellow, target_x=489)\\n    white_error = self.compute_error(mask=mask_white, target_x=100)\\nelse:\\n    yellow_error = self.compute_error(mask=mask_yellow, target_x=100)\\n    white_error = self.compute_error(mask=mask_white, target_x=489)\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This effectively inverts the lane-following algorithm, causing the robot to switch to the left lane while maintaining its ability to follow the lane markers.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our maneuvering approach leveraged our existing lane-following system with key additions:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Target position inversion\"}],\": Dynamically switched target positions for yellow and white lane markers\",\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"# Normal vs. overtaking lane targets\\nyellow_error = self.compute_error(mask=mask_yellow, target_x=100)  # Normal: yellow on left\\nyellow_error = self.compute_error(mask=mask_yellow, target_x=489)  # Overtaking: yellow on right\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Sequential execution\"}],\": Three-phase process (detection → assessment → avoidance)\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"PID controller reuse\"}],\": Same controller with different targets for smooth transitions\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"This approach ensured stable lane changes while leveraging our well-tuned lane following system\"}],\"\\n\"]}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex4/safe_navigation.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This video showcases our Duckiebot's obstacle avoidance capabilities. It begins with the robot driving straight until it detects a broken-down Duckiebot in its path. Upon detection, our robot stops at a safe distance and pauses for 3 seconds. Then, it executes the lane change maneuver, navigating into the opposing lane to safely pass the obstacle. Once it has cleared the broken-down bot, our Duckiebot automatically returns to the original right lane and continues driving.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Reflection\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We found this exercise very insightful, as we got the opportunity to work with AprilTag detection and advanced navigation behaviors. The integration of multiple systems presented unique challenges that required careful coordination of perception and control components.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Code\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Find the code using the link\\n\",[\"$\",\"a\",null,{\"href\":\"https://github.com/akhadeli/cmput412-akhadeli-code/tree/main/exercise-4\",\"children\":\"Code\"}]]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"References\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://wiki.ros.org/ROS/Tutorials\",\"children\":\"ROS Documentation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://docs.duckietown.com/daffy/devmanual-software/beginner\",\"children\":\"Duckiebot Documentation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://medium.com/@nahmed3536/wheel-odometry-model-for-differential-drive-robotics-91b85a012299\",\"children\":\"Odometry Medium Article\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://david010.medium.com/lane-tracking-via-computer-vision-2acb4c7c1c22\",\"children\":\"Lane Tracking Article\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://docs.opencv.org/\",\"children\":\"OpenCV Documentation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/\",\"children\":\"GeeksForGeeks Article\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://en.wikipedia.org/wiki/PID_controller\",\"children\":\"PID Controller Wikipedia\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.youtube.com/watch?v=y3K6FUgrgXw\",\"children\":\"PID Controllers in Unity\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://en.wikipedia.org/wiki/HSL_and_HSV\",\"children\":\"HSV Color Space Wikipedia\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://github.com/duckietown/lib-dt-apriltags\",\"children\":\"AprilTag Library Documentation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://april.eecs.umich.edu/software/apriltag\",\"children\":\"April Robotics Laboratory\"}]}],\"\\n\"]}]]}]}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cmput412-akhadeli/_next/static/css/e23141aac3fba32d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L8\",null,{\"children\":\"$L9\"}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"8qbAYkTHnR8S_v1Yep-pl\",{\"children\":[[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"CMPUT 412/503 - Abdullah Khadeli\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"A documentation site for CMPUT 412/503 - Abdullah Khadeli\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/cmput412-akhadeli/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script></body></html>