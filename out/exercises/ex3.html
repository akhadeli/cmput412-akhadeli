<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/cmput412-akhadeli/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/cmput412-akhadeli/images/ex3/distorted.png"/><link rel="preload" as="image" href="/cmput412-akhadeli/images/ex3/undistorted.png"/><link rel="preload" as="image" href="/cmput412-akhadeli/images/ex3/blue.png"/><link rel="preload" as="image" href="/cmput412-akhadeli/images/ex3/red.png"/><link rel="preload" as="image" href="/cmput412-akhadeli/images/ex3/green.png"/><link rel="stylesheet" href="/cmput412-akhadeli/_next/static/css/f8c155f11af27ff1.css" data-precedence="next"/><link rel="stylesheet" href="/cmput412-akhadeli/_next/static/css/e23141aac3fba32d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/cmput412-akhadeli/_next/static/chunks/webpack-9df4f922c7104209.js"/><script src="/cmput412-akhadeli/_next/static/chunks/4bd1b696-9924fae48e609361.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/517-88af3900d71f33ad.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/main-app-8535f9fd3beb3af1.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/173-7be02dd55462ff0c.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/678-9555f6ed956a8d40.js" async=""></script><script src="/cmput412-akhadeli/_next/static/chunks/app/layout-da0fd956609a208e.js" async=""></script><meta name="next-size-adjust" content=""/><title>CMPUT 412/503 - Abdullah Khadeli</title><meta name="description" content="A documentation site for CMPUT 412/503 - Abdullah Khadeli"/><link rel="icon" href="/cmput412-akhadeli/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/cmput412-akhadeli/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_23f766 antialiased"><div style="--sidebar-width:16rem;--sidebar-width-icon:3rem" class="group/sidebar-wrapper flex min-h-svh w-full has-[[data-variant=inset]]:bg-sidebar"><div class="group peer hidden text-sidebar-foreground md:block" data-state="expanded" data-collapsible="" data-variant="sidebar" data-side="left"><div class="relative h-svh w-[--sidebar-width] bg-transparent transition-[width] duration-200 ease-linear group-data-[collapsible=offcanvas]:w-0 group-data-[side=right]:rotate-180 group-data-[collapsible=icon]:w-[--sidebar-width-icon]"></div><div class="fixed inset-y-0 z-10 hidden h-svh w-[--sidebar-width] transition-[left,right,width] duration-200 ease-linear md:flex left-0 group-data-[collapsible=offcanvas]:left-[calc(var(--sidebar-width)*-1)] group-data-[collapsible=icon]:w-[--sidebar-width-icon] group-data-[side=left]:border-r group-data-[side=right]:border-l"><div data-sidebar="sidebar" class="flex h-full w-full flex-col bg-sidebar group-data-[variant=floating]:rounded-lg group-data-[variant=floating]:border group-data-[variant=floating]:border-sidebar-border group-data-[variant=floating]:shadow"><div data-sidebar="header" class="flex flex-col gap-2 p-2"><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="lg" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-12 text-sm group-data-[collapsible=icon]:!p-0" href="/cmput412-akhadeli"><div class="bg-primary text-primary-foreground flex aspect-square size-8 items-center justify-center rounded-lg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-gallery-vertical-end size-4"><path d="M7 2h10"></path><path d="M5 6h14"></path><rect width="18" height="12" x="3" y="10" rx="2"></rect></svg></div><div class="flex flex-col gap-0.5 leading-none"><span class="font-semibold">CMPUT 412 - akhadeli</span><span class="">v1.0.0</span></div></a></li></ul></div><div data-sidebar="content" class="flex min-h-0 flex-1 flex-col gap-2 overflow-auto group-data-[collapsible=icon]:overflow-hidden"><div data-sidebar="group" class="relative flex w-full min-w-0 flex-col p-2"><div data-sidebar="group-label" class="flex h-8 shrink-0 items-center rounded-md px-2 text-xs font-medium text-sidebar-foreground/70 outline-none ring-sidebar-ring transition-[margin,opa] duration-200 ease-linear focus-visible:ring-2 [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 group-data-[collapsible=icon]:-mt-8 group-data-[collapsible=icon]:opacity-0">Navigation</div><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli"><span>Home</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="https://github.com/akhadeli/cmput412-akhadeli-code"><span>Repository</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/contact"><span>Contact</span></a></li></ul></div><div data-sidebar="group" class="relative flex w-full min-w-0 flex-col p-2"><div data-sidebar="group-label" class="flex h-8 shrink-0 items-center rounded-md px-2 text-xs font-medium text-sidebar-foreground/70 outline-none ring-sidebar-ring transition-[margin,opa] duration-200 ease-linear focus-visible:ring-2 [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 group-data-[collapsible=icon]:-mt-8 group-data-[collapsible=icon]:opacity-0">Exercises</div><ul data-sidebar="menu" class="flex w-full min-w-0 flex-col gap-1"><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/exercises/ex1"><span>Exercise 1</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/exercises/ex2"><span>Exercise 2</span></a></li><li data-sidebar="menu-item" class="group/menu-item relative"><a data-sidebar="menu-button" data-size="default" data-active="false" class="peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left outline-none ring-sidebar-ring transition-[width,height,padding] focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&amp;&gt;span:last-child]:truncate [&amp;&gt;svg]:size-4 [&amp;&gt;svg]:shrink-0 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground h-8 text-sm" href="/cmput412-akhadeli/exercises/ex3"><span>Exercise 3</span></a></li></ul></div></div></div></div></div><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-7 w-7 ml-3 mt-3" data-sidebar="trigger"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-panel-left"><rect width="18" height="18" x="3" y="3" rx="2"></rect><path d="M9 3v18"></path></svg><span class="sr-only">Toggle Sidebar</span></button><main class="flex-1 overflow-auto p-8 pt-16"><main class="mx-auto max-w-3xl"><div class="prose prose-headings:mt-8 prose-headings:font-semibold prose-headings:text-black prose-h1:text-5xl prose-h2:text-4xl prose-h3:text-3xl prose-h4:text-2xl prose-h5:text-xl prose-h6:text-lg dark:prose-headings:text-white prose-md prose-p:text-lg prose-p:leading-relaxed prose-p:text-gray-800 dark:prose-p:text-gray-100 prose-table:w-full prose-table:border-collapse prose-table:border-gray-300 prose-table:dark:border-gray-600 prose-table:text-left prose-table:text-sm prose-table:text-gray-800 dark:prose-table:text-gray-100 prose-ul:text-gray-800 dark:prose-ul:text-gray-100 prose-li:text-gray-800 dark:prose-li:text-gray-100 prose-strong:text-gray-800 prose-strong:font-bold dark:prose-strong:text-gray-100 prose-img:w-full prose-img:rounded-lg prose-img:border prose-img:border-border prose-img:dark:border-border prose-video:w-full prose-video:rounded-lg prose-video:border prose-video:border-border prose-video:dark:border-border prose-iframe:w-full prose-iframe:rounded-lg prose-iframe:border prose-iframe:border-border prose-iframe:dark:border-border prose-span:text-gray-800 prose-span:dark:text-gray-100"><h1>CMPUT 412/503 - Exercise 3 Report</h1>
<h2>Abdullah Khadeli and Ryan Rom</h2>
<h3>Part I - Computer Vision</h3>
<h4>Camera distortion</h4>
<p>Distorted image from the Duckiebot camera
<img src="/cmput412-akhadeli/images/ex3/distorted.png" alt="Calibration board (distorted)"/></p>
<hr/>
<p>Undistorted image from the Duckiebot camera
<img src="/cmput412-akhadeli/images/ex3/undistorted.png" alt="Calibration board (undistorted)"/></p>
<p>In the first image, we can see that the calibration board is distorted due to radial and tangential distortion from the camera lens. In the second image, we can see that the lines appear straight as they should be in reality.</p>
<p>(The sheet with the board was slightly bent, which is why the lines are not perfectly straight in the undistorted image, however the table pattern is shown to be straight, showing that the undistortion is working)</p>
<p>Frames from the Duckiebot camera are distorted due to lens imperfections. Converting distorted images to undistorted ones requires using the camera&#x27;s intrinsic parameters:</p>
<ol>
<li>Subscribing to the camera topic to get the distorted image</li>
<li>Obtaining camera intrinsic parameters (focal length, optical center, distortion coefficients) from the ROS topic for camera intrinsics</li>
<li>Using OpenCV&#x27;s undistort function with the obtained parameters</li>
</ol>
<pre><code class="language-python">undistorted_image = cv2.undistort(distorted_image, camera_matrix, dist_coeffs)
</code></pre>
<ol start="4">
<li>Publishing the undistorted image to a new topic</li>
</ol>
<h4>Color detection</h4>
<p>Image from rqt_image_view showing blue contours
<img src="/cmput412-akhadeli/images/ex3/blue.png" alt="rqt_image_view (blue contours)"/></p>
<hr/>
<p>Image from rqt_image_view showing red contours
<img src="/cmput412-akhadeli/images/ex3/red.png" alt="rqt_image_view (red contours)"/></p>
<hr/>
<p>Image from rqt_image_view showing green contours
<img src="/cmput412-akhadeli/images/ex3/green.png" alt="rqt_image_view (green contours)"/></p>
<p>These images demonstrate our color detection algorithm in action. We&#x27;re using contour detection to identify colored lines in the environment:</p>
<ul>
<li>The blue bounding box in the image shows detection of blue tape on the ground</li>
<li>The red bounding box in the image shows detection of red tape on the ground</li>
<li>The green bounding box in the image shows detection of green tape on the ground</li>
</ul>
<p>In each case, we&#x27;ve used the contours to draw a bounding rectangle around the detected colored area and labeled it with the color name. This visualization helps us verify the accuracy of our HSV range selection and determine the position and dimensions of the line in the robot&#x27;s camera plane.</p>
<h3>Color Detection Methodology</h3>
<p>Color detection works by filtering pixels in the HSV (Hue, Saturation, Value) color space, which separates color information from lighting conditions in a more understandable way compared to RGB. Our approach:</p>
<ol>
<li>Convert the camera image from RGB to HSV color space using</li>
</ol>
<pre><code class="language-python">cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
</code></pre>
<ol start="2">
<li>Define HSV range thresholds for each color:<!-- -->
<ul>
<li>Red: Lower [0, 100, 100], Upper [10, 255, 255]</li>
<li>Blue: Lower [100, 150, 0], Upper [140, 255, 255]</li>
<li>Green: Lower [40, 40, 150], Upper [90, 255, 255]</li>
</ul>
</li>
<li>Create binary masks that isolate pixels within those thresholds using:</li>
</ol>
<pre><code class="language-python">cv2.inRange(image, lower_bound, upper_bound)
</code></pre>
<ol start="4">
<li>Apply dilation with a 5x5 kernel to improve connectivity of detected regions</li>
<li>Find contours in the binary mask with RETR_TREE and CHAIN_APPROX_SIMPLE as per the GeeksForGeeks Article using:</li>
</ol>
<pre><code class="language-python">cv2.findContours()
</code></pre>
<ol start="6">
<li>Filter contours by area (must be &gt; 300 pixels) to eliminate small noise and artifacts</li>
<li>Draw bounding rectangles around valid contours and add relevant text labels</li>
</ol>
<p>Tuning HSV parameters required an iterative process:</p>
<ol>
<li>Starting with initial estimates based on standard HSV color ranges</li>
<li>Testing with images captured in the lab environment</li>
<li>Adjusting ranges incrementally to improve detection under existing lighting conditions</li>
<li>Finding thresholds that balance detection sensitivity with resistance to noise</li>
</ol>
<p>Challenges faced:</p>
<ul>
<li>Finding the right HSV values for all colors due to the variability in reflected light on the tape.<!-- -->
<ul>
<li>We had to use broader ranges for detection to account for this variability, which occasionally resulted in false positives that we usually filtered out using the area threshold.</li>
</ul>
</li>
<li>Shadows on the tape also affected detection, as they were picked up as a different color, so we had to account for that when tuning the HSV values.</li>
</ul>
<h3>Color-Based Behavioral Execution and Line Detection</h3>
<p>Video showing the robot&#x27;s behavior when encountering a blue line</p>
<video src="/cmput412-akhadeli/images/ex3/blue_behavior.mp4" controls=""></video>
<hr/>
<p>Video showing the robot&#x27;s behavior when encountering a red line</p>
<video src="/cmput412-akhadeli/images/ex3/red_behavior.mp4" controls=""></video>
<hr/>
<p>Video showing the robot&#x27;s behavior when encountering a green line</p>
<video src="/cmput412-akhadeli/images/ex3/green_behavior.mp4" controls=""></video>
<p>These videos demonstrate our robot&#x27;s behavior when encountering different colored lines:</p>
<ol>
<li>
<p><strong>Blue Line Behavior</strong>: When the robot detects a blue line, it stops for 5 seconds, signals the right-side LEDs (both front and back), and then makes a right turn.</p>
</li>
<li>
<p><strong>Red Line Behavior</strong>: Upon detecting a red line, the robot stops for 5 seconds and then moves straight forward for 60cm.</p>
</li>
<li>
<p><strong>Green Line Behavior</strong>: When the robot encounters a green line, it stops for 5 seconds, signals the left-side LEDs (both front and back), and then makes a left turn.</p>
</li>
</ol>
<p>For implementing these behaviors, we:</p>
<ol>
<li>
<p>Used a pixel-counting approach to determine which color has the highest count in the frame:</p>
<pre><code class="language-python">detected_color_index = np.argmax([self.blue_count, self.red_count, self.green_count])
</code></pre>
</li>
<li>
<p>Used a homography-based distance estimation to determine how far the robot is from the detected line:</p>
<pre><code class="language-python">def get_distance_from_line(self, colorIndex):
    # Transform image coordinates to real-world coordinates using homography
    homography_matrix, _ = cv2.findHomography(src, dst)
    center = self.get_object_center_from_mask(mask)
    transformed_point = cv2.perspectiveTransform(point_in_image, homography_matrix)
    return transformed_point[0][0][1]  # Returns distance in meters
</code></pre>
</li>
</ol>
<p>To detect lines and determine lane dimensions, we implemented the following approach:</p>
<ol>
<li>
<p><strong>Line Detection</strong>: After creating color masks as described earlier, we use contour detection to identify regions of the detected color.</p>
</li>
<li>
<p><strong>Lane Dimension Determination</strong>:</p>
<ul>
<li>Extract the bounding rectangle of each contour using<!-- -->
<pre><code class="language-python">cv2.boundingRect()
</code></pre>
</li>
<li>Calculate the width, height, and centroid of each rectangle in the frame</li>
<li>Therefore, in relative terms, we can determine if we&#x27;re approaching the line and how far we are from it</li>
</ul>
</li>
</ol>
<h3>Integration</h3>
<p>Our integration of the computer vision, LED control, and wheel movement nodes is as follows:</p>
<ol>
<li>
<p><strong>Node Architecture</strong>:</p>
<ul>
<li>Our <em>ColorDetection</em> node processes images and publishes color masks to the relevant ROS topics.</li>
<li>The <em>BehavioralExecution</em> node or any other executing node subscribes to these color masks and controls robot behavior, and executes its programmed behavior based on the color detected. Additionally, this node will publish LED instructions and wheel commands to the relevant ROS topics.</li>
<li>A wheel control system subscribes to the wheel encoder topic and handles precise movement based on wheel encoder feedback, for example the <em>PIDController</em> node.</li>
<li>An LED controller activates LEDs according to instructions obtained from the topic controlling the LEDs.</li>
</ul>
</li>
<li>
<p><strong>Integration Improvements</strong>:</p>
<ul>
<li>We implemented a sequential task-based architecture where each movement behavior is encapsulated in a task class</li>
<li>To improve our system we could:<!-- -->
<ul>
<li>Use a state based architecture, creating room for RL based approaches (beyond the scope of the exercises)</li>
<li>Allow for parallel execution of tasks</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Optimization and Delay Handling</strong>:</p>
<ul>
<li>Implement a queue system on top of our task based architecture to handle delays more gracefully</li>
<li>Add default behavior for the robot in case of network loss or delay</li>
</ul>
</li>
<li>
<p><strong>Camera Frequency and Update Rate Impact</strong>:</p>
<ul>
<li>
<p>Having a higher camera frequency and control update rate would allow for more precise control of the robot, as we would have more data to work with.</p>
</li>
<li>
<p>However, this would require more compute, which could cause delays and potentially cause the robot to lose control.</p>
<ul>
<li>Especially with the camera frames, since we do some image processing in the node.</li>
<li>We would downscale the image to a lower resolution to reduce processing time, but this would reduce the accuracy of the color detection.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3>Part II - Controllers</h3>
<p>Video showing the robot following a straight lane using a P controller</p>
<video src="/cmput412-akhadeli/images/ex3/P_Controller.mp4" controls=""></video>
<hr/>
<p>Video showing the robot following a straight lane using a PD controller</p>
<video src="/cmput412-akhadeli/images/ex3/PD_Controller.mp4" controls=""></video>
<hr/>
<p>Video showing the robot following a straight lane using a PID controller</p>
<video src="/cmput412-akhadeli/images/ex3/PID_Controller.mp4" controls=""></video>
<p>Each video shows the robot following a straight lane for at least 1.5 meters, demonstrating the performance characteristics of each controller type:</p>
<ol>
<li>
<p><strong>P Controller</strong>: The robot uses a simple proportional controller that adjusts turning based on the error between its current position and the desired lane position.</p>
</li>
<li>
<p><strong>PD Controller</strong>: Building on the P controller, this adds a derivative term that considers how quickly the error is changing, helping to reduce oscillation.</p>
</li>
<li>
<p><strong>PID Controller</strong>: The most complex controller, adding an integral term to the PD controller to address systematic biases and reduce steady-state error.</p>
</li>
</ol>
<h3>Controller Analysis</h3>
<h4>Pros and Cons of Different Controllers</h4>
<p><strong>P (Proportional) Controller</strong>:</p>
<ul>
<li><strong>Pros</strong>:<!-- -->
<ul>
<li>Simple to implement</li>
<li>Computationally efficient</li>
<li>Intuitive tuning</li>
</ul>
</li>
<li><strong>Cons</strong>:<!-- -->
<ul>
<li>Prone to oscillation</li>
<li>Struggles with quick response to large errors</li>
<li>May have steady-state error</li>
</ul>
</li>
</ul>
<p><strong>PD (Proportional-Derivative) Controller</strong>:</p>
<ul>
<li><strong>Pros</strong>:<!-- -->
<ul>
<li>Reduces oscillation</li>
<li>Faster response to changing errors</li>
<li>More stable than P controller</li>
</ul>
</li>
<li><strong>Cons</strong>:<!-- -->
<ul>
<li>More complex tuning</li>
<li>Sensitive to noise</li>
<li>Still may have steady-state error</li>
</ul>
</li>
</ul>
<p><strong>PID (Proportional-Integral-Derivative) Controller</strong>:</p>
<ul>
<li><strong>Pros</strong>:<!-- -->
<ul>
<li>Eliminates steady-state error</li>
<li>Handles external disturbances well</li>
<li>Most robust of the three</li>
</ul>
</li>
<li><strong>Cons</strong>:<!-- -->
<ul>
<li>Most complex to tune</li>
<li>Can introduce instability if tuned incorrectly</li>
<li>Highest compute requirements</li>
</ul>
</li>
</ul>
<h4>Error Calculation</h4>
<p>Our error calculation is based on perspective-transformed lane detection:</p>
<ol>
<li>
<p>We do a perspective transformation to convert the camera view to a bird&#x27;s-eye view:</p>
<pre><code class="language-python">src = np.float32([
    [0,382],
    [224, 191],  # Bottom left (near where left lane line is)
    [589, 382],  # Bottom right (near where right lane line is)
    [364, 191],  # Top left (near vanishing point for left lane)
])

dst = np.float32([
    [100, 382],
    [100, 0],    # Bottom left (destination for left lane)
    [489, 382],  # Bottom right (destination for right lane)
    [489, 0],    # Top left (destination after warping)
])

M = cv2.getPerspectiveTransform(src, dst)
warped = cv2.warpPerspective(image, M, img_size)
</code></pre>
</li>
<li>
<p>We detect both yellow and white lane lines using HSV color filtering:</p>
<pre><code class="language-python"># White lane detection
lower_white = np.array([0, 0, 200], dtype=np.uint8)
upper_white = np.array([180, 50, 255], dtype=np.uint8)
mask_white = cv2.inRange(hsv, lower_white, upper_white)

# Yellow lane detection
lower_yellow = np.array([15, 100, 100], dtype=np.uint8)
upper_yellow = np.array([35, 255, 255], dtype=np.uint8)
mask_yellow = cv2.inRange(hsv, lower_yellow, upper_yellow)
</code></pre>
</li>
<li>
<p>We compute error as the sum of errors between detected lane pixels and their target positions:</p>
<pre><code class="language-python"># The error is the sum of yellow lane errors and white lane errors
error = compute_error(mask=mask_yellow, target_x=100) + compute_error(mask=mask_white, target_x=489)
</code></pre>
</li>
<li>
<p>In the error computation, we find all non-zero pixels in each mask and calculate their distance from the target position:</p>
<pre><code class="language-python"># Find nonzero (active) pixel coordinates
y_coords, x_coords = np.where(mask &gt; 0)

if len(x_coords) == 0:
   return 0  # No detected yellow pixels, return 0 error

# Compute the error as the difference from target_x
errors = (x_coords - target_x) * pixel_value

# Return the sum of the errors
return np.sum(errors)
</code></pre>
</li>
</ol>
<h4>PID Controller Implementation</h4>
<p>Using the error calculated above, the controller adjusts the wheel speeds:</p>
<ol>
<li>
<p><strong>Proportional Term</strong>: Directly proportional to the current error</p>
<pre><code class="language-python">P = self._error * self.proportional_gain
</code></pre>
</li>
<li>
<p><strong>Derivative Term</strong>: Based on the rate of change of error</p>
<pre><code class="language-python">errorRateOfChange = self._error - self._error_last
D = self.derivate_gain * errorRateOfChange
</code></pre>
</li>
<li>
<p><strong>Integral Term</strong>: Based on accumulated error over time, with saturation to prevent integral windup</p>
<pre><code class="language-python">integration_stored_update = self._integration_stored + (self._error)
self._integration_stored = (integration_stored_update) if abs(integration_stored_update) &lt;= self.integral_saturation else (integration_stored_update/integration_stored_update)*self.integral_saturation
I = self.integral_gain * self._integration_stored
</code></pre>
</li>
<li>
<p><strong>Final Control Signal</strong>: The sum of all three terms</p>
<pre><code class="language-python">correction = P + I + D
</code></pre>
</li>
<li>
<p><strong>Correction Update</strong>: We apply the correction by adjusting wheel speeds depending on the direction:</p>
<pre><code class="language-python">if correctionUpdate &lt; 0:
    message = WheelsCmdStamped(vel_left=self.vel, vel_right=self.vel+abs(correctionUpdate))
elif correctionUpdate &gt; 0:
    message = WheelsCmdStamped(vel_left=self.vel+abs(correctionUpdate), vel_right=self.vel)
else:
    message = WheelsCmdStamped(vel_left=self.vel, vel_right=self.vel)
</code></pre>
</li>
</ol>
<h4>Impact of Derivative Term</h4>
<p>The derivative term in our PD controller significantly improved performance by:</p>
<ul>
<li>Reducing oscillations by dampening the control response.</li>
<li>Anticipating the error and correcting for it.</li>
<li>Smoothing transitions between straight sections and curves.</li>
<li>Preventing overshooting when correcting large position errors.</li>
</ul>
<p>We found the derivative term particularly beneficial when the robot needed to make gradual corrections rather than sharp movements, which helped maintain smoother trajectories.</p>
<h4>Impact of Integral Term</h4>
<p>The integral term in our PID controller provided several benefits:</p>
<ul>
<li>Eliminated persistent offset errors that remained with just P and D terms.</li>
<li>Accounted for biases (drift) in our robot&#x27;s movement, such as slight wheel calibration differences.</li>
<li>Improved performance on longer straight sections by gradually correcting small deviations.</li>
<li>Added robustness against uneven surfaces.</li>
</ul>
<p>We implemented integral saturation to prevent the integral term from becoming too large (integral windup), which could cause overcorrection and instability.</p>
<h4>Controller Tuning</h4>
<p>Through systematic testing, we tuned our PID controller parameters:</p>
<ol>
<li>
<p><strong>Tuning Process</strong>:</p>
<ul>
<li>We first tuned the proportional gain until the robot could roughly follow the lane.</li>
<li>Then added derivative gain to reduce oscillations.</li>
<li>Finally added integral gain to eliminate steady-state error.</li>
<li>We used a large integral saturation value to allow for significant error accumulation while preventing extreme integral windup, preventing overshooting.</li>
</ul>
</li>
<li>
<p><strong>Performance Adjustments</strong>:</p>
<ul>
<li>Increased base velocity required higher gains for stable control over uneven surfaces.</li>
</ul>
</li>
<li>
<p><strong>Final Parameters</strong>: We arrived at the following values:</p>
<ul>
<li>Proportional gain: 0.0000002</li>
<li>Derivative gain: 0.0000002</li>
<li>Integral gain: 0.0000002</li>
<li>Base velocity: 0.3</li>
<li>Integral saturation: 500000</li>
</ul>
</li>
</ol>
<p>We found a good balance between responsiveness and stability, allowing the robot to maintain lane position through straight sections and curves with these parameters.</p>
<h3>Part III - Lane following</h3>
<video src="/cmput412-akhadeli/images/ex3/Best_Lane_Following.mp4" controls=""></video>
<p>This video demonstrates our robot performing lane following for a few complete laps around the track using our PID controller. We find this demostration to be a great success, as the robot is able to follow the lane consistently, and is able to navigate the track without veering off the lane.</p>
<p>As we can see in the first curve in the video, the robot did slightly oversteer, but was able to correct itself and stay in the lane. This is a good demonstration of the robot&#x27;s ability to navigate tight curves where we believe the lane lines might partially leave the camera view.</p>
<h4>Lane following - P (Proportional) Controller</h4>
<p>Our basic proportional controller began with a gain of 0.0000002 and a base velocity of 0.3. With just the P controller, the robot could follow the lane but exhibited noticeable oscillation, especially after the turns, where the robot would slightly overshoot the lane.</p>
<p>When encountering large errors (such as when the robot deviates significantly from the center), the P controller would often overshoot, leading to a oscillating &quot;wobbling&quot; behavior. The correction was directly proportional to the error, which meant that larger errors caused more aggressive corrections that often led to overshooting, which created a cycle where the robot would continually overcorrect, making it difficult to stabilize in the lane center.</p>
<h4>Lane following - PD (Proportional-Derivative) Controller</h4>
<p>Adding the derivative term to create a PD controller significantly improved stability. With a derivative gain of 0.0000002, the D term helped dampen the oscillations by considering how quickly the error was changing:</p>
<p>Corrections were smoother and reduced overshooting when dealing with larger errors, such as when recovering from a sharp turn. This approach was particularly effective when transitioning from straight sections to turns, where error values change rapidly.</p>
<h4>Lane following - PID (Proportional-Integral-Derivative) Controller</h4>
<p>With an integral gain of 0.0000002, the I term helped address biases in our robot&#x27;s build, such as slight weight imbalances or wheel calibration differences causing drifiting:</p>
<p>This eliminated the small but persistent offset from the lane center that the PD controller couldn&#x27;t fully resolve after correcting for the error. We implemented integral saturation at 500000 to prevent integral windup, which could have caused instability and overshooting if not handled.</p>
<p>The integral term was particularly helpful for consistent performance over long straight sections where small but persistent errors would otherwise accumulate after the robot had corrected for errors previously.</p>
<h3>Bonus</h3>
<video src="/cmput412-akhadeli/images/ex3/Bonus.mp4" controls=""></video>
<p>This video demonstrates our robot following the lane on the left side of the road, using the English driving system. We modified our lane detection to account for the switch in driving system, and modified the controller to handle the mirrored behavior.</p>
<h3>Reflection</h3>
<p>We found this exercise very insighful, as we got the opportunity to work with PID controllers, which are used in many real world applications in robotics and control systems. We additionally got to coordinate computer vision with control which is a great practical example of how computer vision can be used as feedback for a control system. Addtionally, we gained insight into autonomous navigation and the challenges that come with it, since we had to deal with similar challenges when working on lane following.</p>
<h3>Code</h3>
<p>Find the code using the link
<a href="https://github.com/akhadeli/cmput412-akhadeli-code/tree/main/exercise-3">Code</a></p>
<h3>References</h3>
<ul>
<li><a href="https://wiki.ros.org/ROS/Tutorials">ROS Documentation</a></li>
<li><a href="https://docs.duckietown.com/daffy/devmanual-software/beginner">Duckiebot Documentation</a></li>
<li><a href="https://medium.com/@nahmed3536/wheel-odometry-model-for-differential-drive-robotics-91b85a012299">Odometry Medium Article</a></li>
<li><a href="https://david010.medium.com/lane-tracking-via-computer-vision-2acb4c7c1c22">Lane Tracking Article</a></li>
<li><a href="https://docs.opencv.org/">OpenCV Documentation</a></li>
<li><a href="https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/">GeeksForGeeks Article</a></li>
<li><a href="https://en.wikipedia.org/wiki/PID_controller">PID Controller Wikipedia</a></li>
<li><a href="https://www.youtube.com/watch?v=y3K6FUgrgXw">PID Controllers in Unity</a></li>
<li><a href="https://en.wikipedia.org/wiki/HSL_and_HSV">HSV Color Space Wikipedia</a></li>
</ul></div></main></main></div><script src="/cmput412-akhadeli/_next/static/chunks/webpack-9df4f922c7104209.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[2034,[\"173\",\"static/chunks/173-7be02dd55462ff0c.js\",\"678\",\"static/chunks/678-9555f6ed956a8d40.js\",\"177\",\"static/chunks/app/layout-da0fd956609a208e.js\"],\"SidebarProvider\"]\n3:I[1543,[\"173\",\"static/chunks/173-7be02dd55462ff0c.js\",\"678\",\"static/chunks/678-9555f6ed956a8d40.js\",\"177\",\"static/chunks/app/layout-da0fd956609a208e.js\"],\"AppSidebar\"]\n4:I[2034,[\"173\",\"static/chunks/173-7be02dd55462ff0c.js\",\"678\",\"static/chunks/678-9555f6ed956a8d40.js\",\"177\",\"static/chunks/app/layout-da0fd956609a208e.js\"],\"SidebarTrigger\"]\n5:I[5244,[],\"\"]\n6:I[3866,[],\"\"]\n8:I[6213,[],\"OutletBoundary\"]\na:I[6213,[],\"MetadataBoundary\"]\nc:I[6213,[],\"ViewportBoundary\"]\ne:I[4835,[],\"\"]\n:HL[\"/cmput412-akhadeli/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/cmput412-akhadeli/_next/static/css/f8c155f11af27ff1.css\",\"style\"]\n:HL[\"/cmput412-akhadeli/_next/static/css/e23141aac3fba32d.css\",\"style\"]\n7:T456,prose prose-headings:mt-8 prose-headings:font-semibold prose-headings:text-black prose-h1:text-5xl prose-h2:text-4xl prose-h3:text-3xl prose-h4:text-2xl prose-h5:text-xl prose-h6:text-lg dark:prose-headings:text-white prose-md prose-p:text-lg prose-p:leading-relaxed prose-p:text-gray-800 dark:prose-p:text-gray-100 prose-table:w-full prose-table:border-collapse prose-table:border-gray-300 prose-table:dark:border-gray-600 prose-table:text-left prose-table:text-sm prose-table:text-gray-800 dark:prose-table:text-gray-100 prose-ul:text-gray-800 dark:prose-ul:text-gray-100 prose-li:text-gray-800 dark:prose-li:text-gray-100 prose-strong:text-gray-800 prose-strong:font-bold dark:prose-strong:text-gray-100 prose-img:w-full prose-img:rounded-lg prose-img:border prose-img:border-border prose-img:dark:border-border prose-video:w-full prose-video:rounded-lg prose-video:border prose-video:border-border prose-video:dark:border-border prose-iframe:w-full prose-iframe:rounded-lg prose-iframe:border prose-iframe:border-border prose-iframe:dark:border-border prose-span:text-gray-800 prose-span:dark:"])</script><script>self.__next_f.push([1,"text-gray-100"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"MAsq0nPFXNogvJUWdueT2\",\"p\":\"/cmput412-akhadeli\",\"c\":[\"\",\"exercises\",\"ex3\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"exercises\",{\"children\":[\"ex3\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cmput412-akhadeli/_next/static/css/f8c155f11af27ff1.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"__className_23f766 antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{}],[\"$\",\"$L4\",null,{\"className\":\"ml-3 mt-3\"}],[\"$\",\"main\",null,{\"className\":\"flex-1 overflow-auto p-8 pt-16\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]}]}]]}],{\"children\":[\"exercises\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"exercises\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"ex3\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"exercises\",\"children\",\"ex3\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"main\",null,{\"className\":\"mx-auto max-w-3xl\",\"children\":[\"$\",\"div\",null,{\"className\":\"$7\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"CMPUT 412/503 - Exercise 3 Report\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Abdullah Khadeli and Ryan Rom\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Part I - Computer Vision\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Camera distortion\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Distorted image from the Duckiebot camera\\n\",[\"$\",\"img\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/distorted.png\",\"alt\":\"Calibration board (distorted)\"}]]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Undistorted image from the Duckiebot camera\\n\",[\"$\",\"img\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/undistorted.png\",\"alt\":\"Calibration board (undistorted)\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In the first image, we can see that the calibration board is distorted due to radial and tangential distortion from the camera lens. In the second image, we can see that the lines appear straight as they should be in reality.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"(The sheet with the board was slightly bent, which is why the lines are not perfectly straight in the undistorted image, however the table pattern is shown to be straight, showing that the undistortion is working)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Frames from the Duckiebot camera are distorted due to lens imperfections. Converting distorted images to undistorted ones requires using the camera's intrinsic parameters:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Subscribing to the camera topic to get the distorted image\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Obtaining camera intrinsic parameters (focal length, optical center, distortion coefficients) from the ROS topic for camera intrinsics\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Using OpenCV's undistort function with the obtained parameters\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"undistorted_image = cv2.undistort(distorted_image, camera_matrix, dist_coeffs)\\n\"}]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"4\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Publishing the undistorted image to a new topic\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Color detection\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Image from rqt_image_view showing blue contours\\n\",[\"$\",\"img\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/blue.png\",\"alt\":\"rqt_image_view (blue contours)\"}]]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Image from rqt_image_view showing red contours\\n\",[\"$\",\"img\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/red.png\",\"alt\":\"rqt_image_view (red contours)\"}]]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Image from rqt_image_view showing green contours\\n\",[\"$\",\"img\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/green.png\",\"alt\":\"rqt_image_view (green contours)\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"These images demonstrate our color detection algorithm in action. We're using contour detection to identify colored lines in the environment:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"The blue bounding box in the image shows detection of blue tape on the ground\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The red bounding box in the image shows detection of red tape on the ground\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"The green bounding box in the image shows detection of green tape on the ground\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In each case, we've used the contours to draw a bounding rectangle around the detected colored area and labeled it with the color name. This visualization helps us verify the accuracy of our HSV range selection and determine the position and dimensions of the line in the robot's camera plane.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Color Detection Methodology\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Color detection works by filtering pixels in the HSV (Hue, Saturation, Value) color space, which separates color information from lighting conditions in a more understandable way compared to RGB. Our approach:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Convert the camera image from RGB to HSV color space using\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\\n\"}]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"2\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Define HSV range thresholds for each color:\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Red: Lower [0, 100, 100], Upper [10, 255, 255]\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Blue: Lower [100, 150, 0], Upper [140, 255, 255]\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Green: Lower [40, 40, 150], Upper [90, 255, 255]\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Create binary masks that isolate pixels within those thresholds using:\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"cv2.inRange(image, lower_bound, upper_bound)\\n\"}]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"4\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Apply dilation with a 5x5 kernel to improve connectivity of detected regions\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Find contours in the binary mask with RETR_TREE and CHAIN_APPROX_SIMPLE as per the GeeksForGeeks Article using:\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"cv2.findContours()\\n\"}]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"6\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Filter contours by area (must be \u003e 300 pixels) to eliminate small noise and artifacts\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Draw bounding rectangles around valid contours and add relevant text labels\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Tuning HSV parameters required an iterative process:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Starting with initial estimates based on standard HSV color ranges\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Testing with images captured in the lab environment\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Adjusting ranges incrementally to improve detection under existing lighting conditions\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Finding thresholds that balance detection sensitivity with resistance to noise\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Challenges faced:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Finding the right HSV values for all colors due to the variability in reflected light on the tape.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"We had to use broader ranges for detection to account for this variability, which occasionally resulted in false positives that we usually filtered out using the area threshold.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Shadows on the tape also affected detection, as they were picked up as a different color, so we had to account for that when tuning the HSV values.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Color-Based Behavioral Execution and Line Detection\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Video showing the robot's behavior when encountering a blue line\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/blue_behavior.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Video showing the robot's behavior when encountering a red line\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/red_behavior.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Video showing the robot's behavior when encountering a green line\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/green_behavior.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"These videos demonstrate our robot's behavior when encountering different colored lines:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Blue Line Behavior\"}],\": When the robot detects a blue line, it stops for 5 seconds, signals the right-side LEDs (both front and back), and then makes a right turn.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Red Line Behavior\"}],\": Upon detecting a red line, the robot stops for 5 seconds and then moves straight forward for 60cm.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Green Line Behavior\"}],\": When the robot encounters a green line, it stops for 5 seconds, signals the left-side LEDs (both front and back), and then makes a left turn.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"For implementing these behaviors, we:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Used a pixel-counting approach to determine which color has the highest count in the frame:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"detected_color_index = np.argmax([self.blue_count, self.red_count, self.green_count])\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Used a homography-based distance estimation to determine how far the robot is from the detected line:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"def get_distance_from_line(self, colorIndex):\\n    # Transform image coordinates to real-world coordinates using homography\\n    homography_matrix, _ = cv2.findHomography(src, dst)\\n    center = self.get_object_center_from_mask(mask)\\n    transformed_point = cv2.perspectiveTransform(point_in_image, homography_matrix)\\n    return transformed_point[0][0][1]  # Returns distance in meters\\n\"}]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"To detect lines and determine lane dimensions, we implemented the following approach:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Line Detection\"}],\": After creating color masks as described earlier, we use contour detection to identify regions of the detected color.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Lane Dimension Determination\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Extract the bounding rectangle of each contour using\",\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"cv2.boundingRect()\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Calculate the width, height, and centroid of each rectangle in the frame\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Therefore, in relative terms, we can determine if we're approaching the line and how far we are from it\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Integration\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our integration of the computer vision, LED control, and wheel movement nodes is as follows:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Node Architecture\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Our \",[\"$\",\"em\",null,{\"children\":\"ColorDetection\"}],\" node processes images and publishes color masks to the relevant ROS topics.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The \",[\"$\",\"em\",null,{\"children\":\"BehavioralExecution\"}],\" node or any other executing node subscribes to these color masks and controls robot behavior, and executes its programmed behavior based on the color detected. Additionally, this node will publish LED instructions and wheel commands to the relevant ROS topics.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"A wheel control system subscribes to the wheel encoder topic and handles precise movement based on wheel encoder feedback, for example the \",[\"$\",\"em\",null,{\"children\":\"PIDController\"}],\" node.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"An LED controller activates LEDs according to instructions obtained from the topic controlling the LEDs.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Integration Improvements\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"We implemented a sequential task-based architecture where each movement behavior is encapsulated in a task class\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"To improve our system we could:\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Use a state based architecture, creating room for RL based approaches (beyond the scope of the exercises)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Allow for parallel execution of tasks\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Optimization and Delay Handling\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Implement a queue system on top of our task based architecture to handle delays more gracefully\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Add default behavior for the robot in case of network loss or delay\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Camera Frequency and Update Rate Impact\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Having a higher camera frequency and control update rate would allow for more precise control of the robot, as we would have more data to work with.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"However, this would require more compute, which could cause delays and potentially cause the robot to lose control.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Especially with the camera frames, since we do some image processing in the node.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"We would downscale the image to a lower resolution to reduce processing time, but this would reduce the accuracy of the color detection.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Part II - Controllers\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Video showing the robot following a straight lane using a P controller\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/P_Controller.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Video showing the robot following a straight lane using a PD controller\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/PD_Controller.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Video showing the robot following a straight lane using a PID controller\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/PID_Controller.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Each video shows the robot following a straight lane for at least 1.5 meters, demonstrating the performance characteristics of each controller type:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"P Controller\"}],\": The robot uses a simple proportional controller that adjusts turning based on the error between its current position and the desired lane position.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"PD Controller\"}],\": Building on the P controller, this adds a derivative term that considers how quickly the error is changing, helping to reduce oscillation.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"PID Controller\"}],\": The most complex controller, adding an integral term to the PD controller to address systematic biases and reduce steady-state error.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Controller Analysis\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Pros and Cons of Different Controllers\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"P (Proportional) Controller\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Pros\"}],\":\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Simple to implement\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Computationally efficient\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Intuitive tuning\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cons\"}],\":\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Prone to oscillation\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Struggles with quick response to large errors\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"May have steady-state error\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"PD (Proportional-Derivative) Controller\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Pros\"}],\":\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Reduces oscillation\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Faster response to changing errors\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"More stable than P controller\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cons\"}],\":\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"More complex tuning\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Sensitive to noise\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Still may have steady-state error\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"PID (Proportional-Integral-Derivative) Controller\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Pros\"}],\":\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Eliminates steady-state error\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Handles external disturbances well\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Most robust of the three\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cons\"}],\":\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Most complex to tune\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Can introduce instability if tuned incorrectly\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Highest compute requirements\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Error Calculation\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our error calculation is based on perspective-transformed lane detection:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"We do a perspective transformation to convert the camera view to a bird's-eye view:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"src = np.float32([\\n    [0,382],\\n    [224, 191],  # Bottom left (near where left lane line is)\\n    [589, 382],  # Bottom right (near where right lane line is)\\n    [364, 191],  # Top left (near vanishing point for left lane)\\n])\\n\\ndst = np.float32([\\n    [100, 382],\\n    [100, 0],    # Bottom left (destination for left lane)\\n    [489, 382],  # Bottom right (destination for right lane)\\n    [489, 0],    # Top left (destination after warping)\\n])\\n\\nM = cv2.getPerspectiveTransform(src, dst)\\nwarped = cv2.warpPerspective(image, M, img_size)\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"We detect both yellow and white lane lines using HSV color filtering:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"# White lane detection\\nlower_white = np.array([0, 0, 200], dtype=np.uint8)\\nupper_white = np.array([180, 50, 255], dtype=np.uint8)\\nmask_white = cv2.inRange(hsv, lower_white, upper_white)\\n\\n# Yellow lane detection\\nlower_yellow = np.array([15, 100, 100], dtype=np.uint8)\\nupper_yellow = np.array([35, 255, 255], dtype=np.uint8)\\nmask_yellow = cv2.inRange(hsv, lower_yellow, upper_yellow)\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"We compute error as the sum of errors between detected lane pixels and their target positions:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"# The error is the sum of yellow lane errors and white lane errors\\nerror = compute_error(mask=mask_yellow, target_x=100) + compute_error(mask=mask_white, target_x=489)\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"In the error computation, we find all non-zero pixels in each mask and calculate their distance from the target position:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"# Find nonzero (active) pixel coordinates\\ny_coords, x_coords = np.where(mask \u003e 0)\\n\\nif len(x_coords) == 0:\\n   return 0  # No detected yellow pixels, return 0 error\\n\\n# Compute the error as the difference from target_x\\nerrors = (x_coords - target_x) * pixel_value\\n\\n# Return the sum of the errors\\nreturn np.sum(errors)\\n\"}]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"PID Controller Implementation\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Using the error calculated above, the controller adjusts the wheel speeds:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Proportional Term\"}],\": Directly proportional to the current error\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"P = self._error * self.proportional_gain\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Derivative Term\"}],\": Based on the rate of change of error\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"errorRateOfChange = self._error - self._error_last\\nD = self.derivate_gain * errorRateOfChange\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Integral Term\"}],\": Based on accumulated error over time, with saturation to prevent integral windup\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"integration_stored_update = self._integration_stored + (self._error)\\nself._integration_stored = (integration_stored_update) if abs(integration_stored_update) \u003c= self.integral_saturation else (integration_stored_update/integration_stored_update)*self.integral_saturation\\nI = self.integral_gain * self._integration_stored\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Final Control Signal\"}],\": The sum of all three terms\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"correction = P + I + D\\n\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Correction Update\"}],\": We apply the correction by adjusting wheel speeds depending on the direction:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-python\",\"children\":\"if correctionUpdate \u003c 0:\\n    message = WheelsCmdStamped(vel_left=self.vel, vel_right=self.vel+abs(correctionUpdate))\\nelif correctionUpdate \u003e 0:\\n    message = WheelsCmdStamped(vel_left=self.vel+abs(correctionUpdate), vel_right=self.vel)\\nelse:\\n    message = WheelsCmdStamped(vel_left=self.vel, vel_right=self.vel)\\n\"}]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Impact of Derivative Term\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The derivative term in our PD controller significantly improved performance by:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Reducing oscillations by dampening the control response.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Anticipating the error and correcting for it.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Smoothing transitions between straight sections and curves.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Preventing overshooting when correcting large position errors.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We found the derivative term particularly beneficial when the robot needed to make gradual corrections rather than sharp movements, which helped maintain smoother trajectories.\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Impact of Integral Term\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The integral term in our PID controller provided several benefits:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Eliminated persistent offset errors that remained with just P and D terms.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Accounted for biases (drift) in our robot's movement, such as slight wheel calibration differences.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Improved performance on longer straight sections by gradually correcting small deviations.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Added robustness against uneven surfaces.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We implemented integral saturation to prevent the integral term from becoming too large (integral windup), which could cause overcorrection and instability.\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Controller Tuning\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Through systematic testing, we tuned our PID controller parameters:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Tuning Process\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"We first tuned the proportional gain until the robot could roughly follow the lane.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Then added derivative gain to reduce oscillations.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Finally added integral gain to eliminate steady-state error.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"We used a large integral saturation value to allow for significant error accumulation while preventing extreme integral windup, preventing overshooting.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Performance Adjustments\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Increased base velocity required higher gains for stable control over uneven surfaces.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Final Parameters\"}],\": We arrived at the following values:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Proportional gain: 0.0000002\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Derivative gain: 0.0000002\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Integral gain: 0.0000002\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Base velocity: 0.3\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Integral saturation: 500000\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We found a good balance between responsiveness and stability, allowing the robot to maintain lane position through straight sections and curves with these parameters.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Part III - Lane following\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/Best_Lane_Following.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This video demonstrates our robot performing lane following for a few complete laps around the track using our PID controller. We find this demostration to be a great success, as the robot is able to follow the lane consistently, and is able to navigate the track without veering off the lane.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"As we can see in the first curve in the video, the robot did slightly oversteer, but was able to correct itself and stay in the lane. This is a good demonstration of the robot's ability to navigate tight curves where we believe the lane lines might partially leave the camera view.\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Lane following - P (Proportional) Controller\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our basic proportional controller began with a gain of 0.0000002 and a base velocity of 0.3. With just the P controller, the robot could follow the lane but exhibited noticeable oscillation, especially after the turns, where the robot would slightly overshoot the lane.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"When encountering large errors (such as when the robot deviates significantly from the center), the P controller would often overshoot, leading to a oscillating \\\"wobbling\\\" behavior. The correction was directly proportional to the error, which meant that larger errors caused more aggressive corrections that often led to overshooting, which created a cycle where the robot would continually overcorrect, making it difficult to stabilize in the lane center.\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Lane following - PD (Proportional-Derivative) Controller\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Adding the derivative term to create a PD controller significantly improved stability. With a derivative gain of 0.0000002, the D term helped dampen the oscillations by considering how quickly the error was changing:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Corrections were smoother and reduced overshooting when dealing with larger errors, such as when recovering from a sharp turn. This approach was particularly effective when transitioning from straight sections to turns, where error values change rapidly.\"}],\"\\n\",[\"$\",\"h4\",null,{\"children\":\"Lane following - PID (Proportional-Integral-Derivative) Controller\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"With an integral gain of 0.0000002, the I term helped address biases in our robot's build, such as slight weight imbalances or wheel calibration differences causing drifiting:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This eliminated the small but persistent offset from the lane center that the PD controller couldn't fully resolve after correcting for the error. We implemented integral saturation at 500000 to prevent integral windup, which could have caused instability and overshooting if not handled.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The integral term was particularly helpful for consistent performance over long straight sections where small but persistent errors would otherwise accumulate after the robot had corrected for errors previously.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Bonus\"}],\"\\n\",[\"$\",\"video\",null,{\"src\":\"/cmput412-akhadeli/images/ex3/Bonus.mp4\",\"controls\":true}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This video demonstrates our robot following the lane on the left side of the road, using the English driving system. We modified our lane detection to account for the switch in driving system, and modified the controller to handle the mirrored behavior.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Reflection\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We found this exercise very insighful, as we got the opportunity to work with PID controllers, which are used in many real world applications in robotics and control systems. We additionally got to coordinate computer vision with control which is a great practical example of how computer vision can be used as feedback for a control system. Addtionally, we gained insight into autonomous navigation and the challenges that come with it, since we had to deal with similar challenges when working on lane following.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Code\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Find the code using the link\\n\",[\"$\",\"a\",null,{\"href\":\"https://github.com/akhadeli/cmput412-akhadeli-code/tree/main/exercise-3\",\"children\":\"Code\"}]]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"References\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://wiki.ros.org/ROS/Tutorials\",\"children\":\"ROS Documentation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://docs.duckietown.com/daffy/devmanual-software/beginner\",\"children\":\"Duckiebot Documentation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://medium.com/@nahmed3536/wheel-odometry-model-for-differential-drive-robotics-91b85a012299\",\"children\":\"Odometry Medium Article\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://david010.medium.com/lane-tracking-via-computer-vision-2acb4c7c1c22\",\"children\":\"Lane Tracking Article\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://docs.opencv.org/\",\"children\":\"OpenCV Documentation\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/\",\"children\":\"GeeksForGeeks Article\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://en.wikipedia.org/wiki/PID_controller\",\"children\":\"PID Controller Wikipedia\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.youtube.com/watch?v=y3K6FUgrgXw\",\"children\":\"PID Controllers in Unity\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://en.wikipedia.org/wiki/HSL_and_HSV\",\"children\":\"HSV Color Space Wikipedia\"}]}],\"\\n\"]}]]}]}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cmput412-akhadeli/_next/static/css/e23141aac3fba32d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L8\",null,{\"children\":\"$L9\"}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"NmBgfhsqk6xScrhybS1bt\",{\"children\":[[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"CMPUT 412/503 - Abdullah Khadeli\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"A documentation site for CMPUT 412/503 - Abdullah Khadeli\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/cmput412-akhadeli/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script></body></html>