# CMPUT 412/503 - Exercise 2 Report

## Abdullah Khadeli and Ryan Rom

### Part I: Getting comfortable with ROS

- **Node**: An executable file within a ROS package. Each node should be built to perform specific tasks like controlling sensors or motors, and communicating with other nodes by publishing and subscribing to topics.

- **Topic**: Topics are a way to exchange mnessages betwee nodes by using a pubsub model. Publishers send messages to topics, while subscribers receive them, allowing for decoupled communication.

- **Service**: Services provide an alternative communication mechanism between nodes. Unlike the asynchronous nature of topics, services are for operations requiring a request-response interaction, like querying sensor states.

- **Message**: Messages provide typed data schemas for node communication. They provide support for different types of data exchanged between nodes.

- **Bag**: A method to record messages sent through a topic. Since timestamps are attached to each message, it allows for the playback of the messages in the order they were sent, and thus analysis of the time-series data.

#### Typical workflow for communication

The typical workflow for setting up node communication involves:

1. Creating your nodes
2. Defining message schemas using the built-in message types
3. Attaching publishers and subscribers to the appropriate topics
4. Launching the respective nodes
5. Additionally collect data using rosbag for playback and analysis

The following images show my-publisher.py publishing messages and my-subscriber.py reading the published messages running from the duckiebot.

![Publisher and Subscriber](/cmput412-akhadeli/images/ex2/pub-sub-simple.png)

Afterwards, we subscribed to the topic containing the output of the camera.
We then transformed the read image into grayscale and annotated it using OpenCV and published the compressed output on a custom topic.
We opened rqt_image_view and used the outputs from the custom topic showing this screenshot.

![Annotated Image](/cmput412-akhadeli/images/ex2/annotated-image.png)

### Part II: Odometry using wheel encoders

#### Straight line task

To achieve the straight line task, we set a target distance to travel, and using the tick and resolution
information provided by the motor encoders, and the wheel radius, we can calculate the current distance travelled,
and therefore terminate the event loop when the current distance travelled is within a tolerance threshold of the target distance.

<video src="/cmput412-akhadeli/images/ex2/straight.mp4" controls></video>

Here is a video of the robot driving in a straight line forwards and backwards.

Most of the time the bot stops near exact distance (1.25m) but sometimes it goes way over it.
We believe that there are a few reasons for this:

1. Lag in the connection between the messages that the publisher and subscriber communicate with, imparted due to the high refresh rate we forced the robot to run at.
2. The uneven surface of the mat leads to drifting away from the desired path.
3. Observer encoder inaccuracies when the robot is driving on the surface, likely due to the weight of the robot.

```
WheelsCMDStamped(vel_left=0, vel_right=0)
```

Furthermore, the uneven surface of the mat leads to drift into one direction.

We found that setting the speed to full throttle is the best option to compensate for the unevenness of the surface, therefore we applied full throttle to the robot.

Increasing the speed leads to more accurate results in our case.
This is likely because the mat used for the robot has been tampered a lot over 2 weeks as other students use it for their testing and a lot of accidental stepping on mat has happened, creating unevenness.
Decreasing the speed leads to less accurate results like for the same reason expressed previously.

![Straight Plot](/cmput412-akhadeli/images/ex2/straight_plot.png)

#### Rotation task

To achieve the rotation task, we set a target angle to rotate to, and using the tick and resolution
information provided by the motor encoders, and the wheel radius, we can calculate the current angle of rotation,

```math
\Delta \theta = \frac{d_R - d_L}{2d_w}
```

and therefore terminate the event loop when the current angle of rotation is within a tolerance threshold of the target angle.

<video src="/cmput412-akhadeli/images/ex2/rotation.mp4" controls></video>

One of the most pressing problems we faced in the rotation task is overshooting.
After spending a long time tweaking our implementation, we achieved near exact 90 degree rotation in most of our runs.
However, sometimes it overshoots and, rarely, its spins around more than 360 degrees.
It mainly came from the lag and number of published messages per second.
There was nothing we can do about the former. For the latter, we set a tolerance threshold such that when the read that total change in angle of the robot frame is within that lower threshold we stop the robot.
As well, we increased the number of published messages per second. This makes the interval of error really small so it only overshoots a little if we ever do and there is no lag.

Our programs did not finish properly in the beginning.
When we shutdown our program for the first time, the robot's wheels kept spinning even though the run function already finished.
We, however, quickly realized this after looking at the sample exercises (E.g. camera reader).
We defined an on_shutdown() callback to our node that publishes a command to the wheels of the robot that set the velocity of each wheel to 0.
After we defined this in our DTROS subclasses, our nodes shutdown properly.

![Rotation Plot](/cmput412-akhadeli/images/ex2/rotation_plot.png)

### Part III: Playing with Duckiebots

After building our main node and service nodes and hours of testing and tweaking their parameters, we achieved our best run on running the Duckiebot to traced a D path.

<video src="/cmput412-akhadeli/images/ex2/d-path-2.mp4" controls></video>

The main node is responsible for the logic of controlling the wheels of the duckiebot.
It also publishes a msg to /csc22909/state. The service node subscribes to this topic.
There are three states Stopping, Moving, and Exiting. The service node handles the lights to output **Red** meaning the duckiebot is in a **stopped state** and **Green** means that the duckiebot is **moving**.
When the node shuts down, we turn the lights off.

![D Path Plot](/cmput412-akhadeli/images/ex2/D_plot.png)

![Total Execution Time](/cmput412-akhadeli/images/ex2/exec-time.png)

Accumulation of small errors leading to big errors in the long run (E.g. minor deviation from the first straight line task in the D path tracing leads to the robot not reaching the end position)
It is also very hard to get accurate movement from the robot because of the reasons stated in the previous parts.

### Reflection

Overall, we learned a lot about ROS and Duckiebots, and especially about tweaking the parameters of the robot to get the best performance.

### References

- [ROS Documentation](https://wiki.ros.org/ROS/Tutorials)
- [Duckiebot Documentation](https://docs.duckietown.com/daffy/devmanual-software/beginner)
- [Medium Article](https://medium.com/@nahmed3536/wheel-odometry-model-for-differential-drive-robotics-91b85a012299)
