# CMPUT 412/503 - Exercise 3 Report

## Abdullah Khadeli and Ryan Rom

### Part I - Computer Vision

#### Camera distortion

Distorted image from the Duckiebot camera
![Calibration board (distorted)](/cmput412-akhadeli/images/ex3/distorted.png)

<hr />

Undistorted image from the Duckiebot camera
![Calibration board (undistorted)](/cmput412-akhadeli/images/ex3/undistorted.png)

In the first image, we can see that the calibration board is distorted due to radial and tangential distortion from the camera lens. In the second image, we can see that the lines appear straight as they should be in reality.

(The sheet with the board was slightly bent, which is why the lines are not perfectly straight in the undistorted image, however the table pattern is shown to be straight, showing that the undistortion is working)

Frames from the Duckiebot camera are distorted due to lens imperfections. Converting distorted images to undistorted ones requires using the camera's intrinsic parameters:

1. Subscribing to the camera topic to get the distorted image
2. Obtaining camera intrinsic parameters (focal length, optical center, distortion coefficients) from the ROS topic for camera intrinsics
3. Using OpenCV's undistort function with the obtained parameters

```python
undistorted_image = cv2.undistort(distorted_image, camera_matrix, dist_coeffs)
```

4. Publishing the undistorted image to a new topic

#### Color detection

Image from rqt_image_view showing blue contours
![rqt_image_view (blue contours)](/cmput412-akhadeli/images/ex3/blue.png)

<hr />

Image from rqt_image_view showing red contours
![rqt_image_view (red contours)](/cmput412-akhadeli/images/ex3/red.png)

<hr />

Image from rqt_image_view showing green contours
![rqt_image_view (green contours)](/cmput412-akhadeli/images/ex3/green.png)

These images demonstrate our color detection algorithm in action. We're using contour detection to identify colored lines in the environment:

- The blue bounding box in the image shows detection of blue tape on the ground
- The red bounding box in the image shows detection of red tape on the ground
- The green bounding box in the image shows detection of green tape on the ground

In each case, we've used the contours to draw a bounding rectangle around the detected colored area and labeled it with the color name. This visualization helps us verify the accuracy of our HSV range selection and determine the position and dimensions of the line in the robot's camera plane.

### Color Detection Methodology

Color detection works by filtering pixels in the HSV (Hue, Saturation, Value) color space, which separates color information from lighting conditions in a more understandable way compared to RGB. Our approach:

1. Convert the camera image from RGB to HSV color space using

```python
cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
```

2. Define HSV range thresholds for each color:
   - Red: Lower [0, 100, 100], Upper [10, 255, 255]
   - Blue: Lower [100, 150, 0], Upper [140, 255, 255]
   - Green: Lower [40, 40, 150], Upper [90, 255, 255]
3. Create binary masks that isolate pixels within those thresholds using:

```python
cv2.inRange(image, lower_bound, upper_bound)
```

4. Apply dilation with a 5x5 kernel to improve connectivity of detected regions
5. Find contours in the binary mask with RETR_TREE and CHAIN_APPROX_SIMPLE as per the GeeksForGeeks Article using:

```python
cv2.findContours()
```

6. Filter contours by area (must be > 300 pixels) to eliminate small noise and artifacts
7. Draw bounding rectangles around valid contours and add relevant text labels

Tuning HSV parameters required an iterative process:

1. Starting with initial estimates based on standard HSV color ranges
2. Testing with images captured in the lab environment
3. Adjusting ranges incrementally to improve detection under existing lighting conditions
4. Finding thresholds that balance detection sensitivity with resistance to noise

Challenges faced:

- Finding the right HSV values for all colors due to the variability in reflected light on the tape.
  - We had to use broader ranges for detection to account for this variability, which occasionally resulted in false positives that we usually filtered out using the area threshold.
- Shadows on the tape also affected detection, as they were picked up as a different color, so we had to account for that when tuning the HSV values.

### Color-Based Behavioral Execution and Line Detection

Video showing the robot's behavior when encountering a blue line

<video src="/cmput412-akhadeli/images/ex3/blue_behavior.mp4" controls></video>

<hr />

Video showing the robot's behavior when encountering a red line

<video src="/cmput412-akhadeli/images/ex3/red_behavior.mp4" controls></video>

<hr />

Video showing the robot's behavior when encountering a green line

<video src="/cmput412-akhadeli/images/ex3/green_behavior.mp4" controls></video>

These videos demonstrate our robot's behavior when encountering different colored lines:

1. **Blue Line Behavior**: When the robot detects a blue line, it stops for 5 seconds, signals the right-side LEDs (both front and back), and then makes a right turn.

2. **Red Line Behavior**: Upon detecting a red line, the robot stops for 5 seconds and then moves straight forward for 60cm.

3. **Green Line Behavior**: When the robot encounters a green line, it stops for 5 seconds, signals the left-side LEDs (both front and back), and then makes a left turn.

For implementing these behaviors, we:

1. Used a pixel-counting approach to determine which color has the highest count in the frame:

   ```python
   detected_color_index = np.argmax([self.blue_count, self.red_count, self.green_count])
   ```

2. Used a homography-based distance estimation to determine how far the robot is from the detected line:

   ```python
   def get_distance_from_line(self, colorIndex):
       # Transform image coordinates to real-world coordinates using homography
       homography_matrix, _ = cv2.findHomography(src, dst)
       center = self.get_object_center_from_mask(mask)
       transformed_point = cv2.perspectiveTransform(point_in_image, homography_matrix)
       return transformed_point[0][0][1]  # Returns distance in meters
   ```

To detect lines and determine lane dimensions, we implemented the following approach:

1. **Line Detection**: After creating color masks as described earlier, we use contour detection to identify regions of the detected color.

2. **Lane Dimension Determination**:

   - Extract the bounding rectangle of each contour using
     ```python
     cv2.boundingRect()
     ```
   - Calculate the width, height, and centroid of each rectangle in the frame
   - Therefore, in relative terms, we can determine if we're approaching the line and how far we are from it

### Integration

Our integration of the computer vision, LED control, and wheel movement nodes is as follows:

1. **Node Architecture**:

   - Our _ColorDetection_ node processes images and publishes color masks to the relevant ROS topics.
   - The _BehavioralExecution_ node or any other executing node subscribes to these color masks and controls robot behavior, and executes its programmed behavior based on the color detected. Additionally, this node will publish LED instructions and wheel commands to the relevant ROS topics.
   - A wheel control system subscribes to the wheel encoder topic and handles precise movement based on wheel encoder feedback, for example the _PIDController_ node.
   - An LED controller activates LEDs according to instructions obtained from the topic controlling the LEDs.

2. **Integration Improvements**:

   - We implemented a sequential task-based architecture where each movement behavior is encapsulated in a task class
   - To improve our system we could:
     - Use a state based architecture, creating room for RL based approaches (beyond the scope of the exercises)
     - Allow for parallel execution of tasks

3. **Optimization and Delay Handling**:

   - Implement a queue system on top of our task based architecture to handle delays more gracefully
   - Add default behavior for the robot in case of network loss or delay

4. **Camera Frequency and Update Rate Impact**:

   - Having a higher camera frequency and control update rate would allow for more precise control of the robot, as we would have more data to work with.
   - However, this would require more compute, which could cause delays and potentially cause the robot to lose control.

     - Especially with the camera frames, since we do some image processing in the node.
     - We would downscale the image to a lower resolution to reduce processing time, but this would reduce the accuracy of the color detection.

### Part II - Controllers

Video showing the robot following a straight lane using a P controller

<video src="/cmput412-akhadeli/images/ex3/P_Controller.mp4" controls></video>

<hr />

Video showing the robot following a straight lane using a PD controller

<video src="/cmput412-akhadeli/images/ex3/PD_Controller.mp4" controls></video>

<hr />

Video showing the robot following a straight lane using a PID controller

<video src="/cmput412-akhadeli/images/ex3/PID_Controller.mp4" controls></video>

Each video shows the robot following a straight lane for at least 1.5 meters, demonstrating the performance characteristics of each controller type:

1. **P Controller**: The robot uses a simple proportional controller that adjusts turning based on the error between its current position and the desired lane position.

- In the video we can see slight oscillations, but the robot is able to follow the lane.

2. **PD Controller**: Building on the P controller, this adds a derivative term that considers how quickly the error is changing, helping to reduce oscillation.

- In the video we can see a strong correction initially, and then smoother corrections for the oscillations.

3. **PID Controller**: The most complex controller, adding an integral term to the PD controller to address systematic biases and reduce steady-state error.

- In the video we can see a good balance between responsiveness and stability, with very minimal oscillations.

### Controller Analysis

#### Pros and Cons of Different Controllers

**P (Proportional) Controller**:

- **Pros**:
  - Simple to implement
  - Computationally efficient
  - Intuitive tuning
- **Cons**:
  - Prone to oscillation
  - Struggles with quick response to large errors
  - May have steady-state error

**PD (Proportional-Derivative) Controller**:

- **Pros**:
  - Reduces oscillation
  - Faster response to changing errors
  - More stable than P controller
- **Cons**:
  - More complex tuning
  - Sensitive to noise
  - Still may have steady-state error

**PID (Proportional-Integral-Derivative) Controller**:

- **Pros**:
  - Eliminates steady-state error
  - Handles external disturbances well
  - Most robust of the three
- **Cons**:
  - Most complex to tune
  - Can introduce instability if tuned incorrectly
  - Highest compute requirements

#### Error Calculation

Our error calculation is based on perspective-transformed lane detection:

1. We do a perspective transformation to convert the camera view to a bird's-eye view:

   ```python
   src = np.float32([
       [0,382],
       [224, 191],  # Bottom left (near where left lane line is)
       [589, 382],  # Bottom right (near where right lane line is)
       [364, 191],  # Top left (near vanishing point for left lane)
   ])

   dst = np.float32([
       [100, 382],
       [100, 0],    # Bottom left (destination for left lane)
       [489, 382],  # Bottom right (destination for right lane)
       [489, 0],    # Top left (destination after warping)
   ])

   M = cv2.getPerspectiveTransform(src, dst)
   warped = cv2.warpPerspective(image, M, img_size)
   ```

2. We detect both yellow and white lane lines using HSV color filtering:

   ```python
   # White lane detection
   lower_white = np.array([0, 0, 200], dtype=np.uint8)
   upper_white = np.array([180, 50, 255], dtype=np.uint8)
   mask_white = cv2.inRange(hsv, lower_white, upper_white)

   # Yellow lane detection
   lower_yellow = np.array([15, 100, 100], dtype=np.uint8)
   upper_yellow = np.array([35, 255, 255], dtype=np.uint8)
   mask_yellow = cv2.inRange(hsv, lower_yellow, upper_yellow)
   ```

3. We compute error as the sum of errors between detected lane pixels and their target positions:

   ```python
   # The error is the sum of yellow lane errors and white lane errors
   error = compute_error(mask=mask_yellow, target_x=100) + compute_error(mask=mask_white, target_x=489)
   ```

4. In the error computation, we find all non-zero pixels in each mask and calculate their distance from the target position:

   ```python
   # Find nonzero (active) pixel coordinates
   y_coords, x_coords = np.where(mask > 0)

   if len(x_coords) == 0:
      return 0  # No detected yellow pixels, return 0 error

   # Compute the error as the difference from target_x
   errors = (x_coords - target_x) * pixel_value

   # Return the sum of the errors
   return np.sum(errors)
   ```

#### PID Controller Implementation

Using the error calculated above, the controller adjusts the wheel speeds:

1. **Proportional Term**: Directly proportional to the current error

   ```python
   P = self._error * self.proportional_gain
   ```

2. **Derivative Term**: Based on the rate of change of error

   ```python
   errorRateOfChange = self._error - self._error_last
   D = self.derivate_gain * errorRateOfChange
   ```

3. **Integral Term**: Based on accumulated error over time, with saturation to prevent integral windup

   ```python
   integration_stored_update = self._integration_stored + (self._error)
   self._integration_stored = (integration_stored_update) if abs(integration_stored_update) <= self.integral_saturation else (integration_stored_update/integration_stored_update)*self.integral_saturation
   I = self.integral_gain * self._integration_stored
   ```

4. **Final Control Signal**: The sum of all three terms

   ```python
   correction = P + I + D
   ```

5. **Correction Update**: We apply the correction by adjusting wheel speeds depending on the direction:
   ```python
   if correctionUpdate < 0:
       message = WheelsCmdStamped(vel_left=self.vel, vel_right=self.vel+abs(correctionUpdate))
   elif correctionUpdate > 0:
       message = WheelsCmdStamped(vel_left=self.vel+abs(correctionUpdate), vel_right=self.vel)
   else:
       message = WheelsCmdStamped(vel_left=self.vel, vel_right=self.vel)
   ```

#### Impact of Derivative Term

The derivative term in our PD controller significantly improved performance by:

- Reducing oscillations by dampening the control response.
- Anticipating the error and correcting for it.
- Smoothing transitions between straight sections and curves.
- Preventing overshooting when correcting large position errors.

We found the derivative term particularly beneficial when the robot needed to make gradual corrections rather than sharp movements, which helped maintain smoother trajectories.

#### Impact of Integral Term

The integral term in our PID controller provided several benefits:

- Eliminated persistent offset errors that remained with just P and D terms.
- Accounted for biases (drift) in our robot's movement, such as slight wheel calibration differences.
- Improved performance on longer straight sections by gradually correcting small deviations.
- Added robustness against uneven surfaces.

We implemented integral saturation to prevent the integral term from becoming too large (integral windup), which could cause overcorrection and instability.

#### Controller Tuning

Through systematic testing, we tuned our PID controller parameters:

1. **Tuning Process**:

   - We first tuned the proportional gain until the robot could roughly follow the lane.
   - Then added derivative gain to reduce oscillations.
   - Finally added integral gain to eliminate steady-state error.
   - We used a large integral saturation value to allow for significant error accumulation while preventing extreme integral windup, preventing overshooting.

2. **Performance Adjustments**:

   - Increased base velocity required higher gains for stable control over uneven surfaces.

3. **Final Parameters**: We arrived at the following values:

   - Proportional gain: 0.0000002
   - Derivative gain: 0.0000002
   - Integral gain: 0.0000002
   - Base velocity: 0.3
   - Integral saturation: 500000

We found a good balance between responsiveness and stability, allowing the robot to maintain lane position through straight sections and curves with these parameters.

### Part III - Lane following

<video
  src="/cmput412-akhadeli/images/ex3/Best_Lane_Following.mp4"
  controls
></video>

This video demonstrates our robot performing lane following for a few complete laps around the track using our PID controller. We find this demostration to be a great success, as the robot is able to follow the lane consistently, and is able to navigate the track without veering off the lane.

As we can see in the first curve in the video, the robot did slightly oversteer, but was able to correct itself and stay in the lane. This is a good demonstration of the robot's ability to navigate tight curves where we believe the lane lines might partially leave the camera view.

#### Lane following - P (Proportional) Controller

Our basic proportional controller began with a gain of 0.0000002 and a base velocity of 0.3. With just the P controller, the robot could follow the lane but exhibited noticeable oscillation, especially after the turns, where the robot would slightly overshoot the lane.

When encountering large errors (such as when the robot deviates significantly from the center), the P controller would often overshoot, leading to a oscillating "wobbling" behavior. The correction was directly proportional to the error, which meant that larger errors caused more aggressive corrections that often led to overshooting, which created a cycle where the robot would continually overcorrect, making it difficult to stabilize in the lane center.

#### Lane following - PD (Proportional-Derivative) Controller

Adding the derivative term to create a PD controller significantly improved stability. With a derivative gain of 0.0000002, the D term helped dampen the oscillations by considering how quickly the error was changing:

Corrections were smoother and reduced overshooting when dealing with larger errors, such as when recovering from a sharp turn. This approach was particularly effective when transitioning from straight sections to turns, where error values change rapidly.

#### Lane following - PID (Proportional-Integral-Derivative) Controller

With an integral gain of 0.0000002, the I term helped address biases in our robot's build, such as slight weight imbalances or wheel calibration differences causing drifiting:

This eliminated the small but persistent offset from the lane center that the PD controller couldn't fully resolve after correcting for the error. We implemented integral saturation at 500000 to prevent integral windup, which could have caused instability and overshooting if not handled.

The integral term was particularly helpful for consistent performance over long straight sections where small but persistent errors would otherwise accumulate after the robot had corrected for errors previously.

### Bonus

<video src="/cmput412-akhadeli/images/ex3/Bonus.mp4" controls></video>

This video demonstrates our robot following the lane on the left side of the road, using the English driving system. We modified our lane detection to account for the switch in driving system, and modified the controller to handle the mirrored behavior.

### Reflection

We found this exercise very insighful, as we got the opportunity to work with PID controllers, which are used in many real world applications in robotics and control systems. We additionally got to coordinate computer vision with control which is a great practical example of how computer vision can be used as feedback for a control system. Addtionally, we gained insight into autonomous navigation and the challenges that come with it, since we had to deal with similar challenges when working on lane following.

### Code

Find the code using the link
[Code](https://github.com/akhadeli/cmput412-akhadeli-code/tree/main/exercise-3)

### References

- [ROS Documentation](https://wiki.ros.org/ROS/Tutorials)
- [Duckiebot Documentation](https://docs.duckietown.com/daffy/devmanual-software/beginner)
- [Odometry Medium Article](https://medium.com/@nahmed3536/wheel-odometry-model-for-differential-drive-robotics-91b85a012299)
- [Lane Tracking Article](https://david010.medium.com/lane-tracking-via-computer-vision-2acb4c7c1c22)
- [OpenCV Documentation](https://docs.opencv.org/)
- [GeeksForGeeks Article](https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/)
- [PID Controller Wikipedia](https://en.wikipedia.org/wiki/PID_controller)
- [PID Controllers in Unity](https://www.youtube.com/watch?v=y3K6FUgrgXw)
- [HSV Color Space Wikipedia](https://en.wikipedia.org/wiki/HSL_and_HSV)
